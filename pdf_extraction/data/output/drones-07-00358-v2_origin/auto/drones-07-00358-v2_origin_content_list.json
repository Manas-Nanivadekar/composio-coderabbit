[
    {
        "type": "text",
        "text": "Article ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "OwlFusion: Depth-Only Onboard Real-Time 3D Reconstruction of Scalable Scenes for Fast-Moving MAV ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Guohua Gou $\\oplus ,$ , Xuanhao Wang, Haigang Sui \\*, Sheng Wang, Hao Zhang and Jiajie Li $\\textcircled{1}$ ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Citation: Gou, G.; Wang, X.; Sui, H.; Wang, S.; Zhang, $\\mathrm { H . } ;$ Li, J. OwlFusion: Depth-Only Onboard Real-Time 3D Reconstruction of Scalable Scenes for Fast-Moving MAV. Drones 2023, 7, 358. https://doi.org/10.3390/ drones7060358 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Academic Editor: Higinio González Jorge ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Received: 3 May 2023   \nRevised: 20 May 2023   \nAccepted: 26 May 2023   \nPublished: 29 May 2023 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Copyright: $^ ©$ 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "State Key Laboratory Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan 430070, China; guohua.gou@whu.edu.cn (G.G.); xuanh.w@whu.edu.cn (X.W.); wsheng@whu.edu.cn (S.W.); zhanghao.1003@whu.edu.cn (H.Z.); 2017301200143@whu.edu.cn (J.L.)   \n\\* Correspondence: 00201543@whu.edu.cn ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract: Real-time 3D reconstruction combined with MAVs has garnered significant attention in a variety of fields, including building maintenance, geological exploration, emergency rescue, and cultural heritage protection. While MAVs possess the advantages of speed and lightness, they also exhibit strong image blur and limited computational resources. To address these limitations, this paper presents a novel approach for onboard, depth-only, real-time 3D reconstruction capable of accommodating fast-moving MAVs. Our primary contribution is a dense SLAM system that combines surface hierarchical sparse representation and particle swarm pose optimization. Our system enables the robust tracking of high-speed camera motion and facilitates scaling to large scenes without being constrained by GPU memory resources. Our robust camera tracking framework is capable of accommodating fast camera motions and varying environments solely by relying on depth images. Furthermore, by integrating path planning methods, we explore the capabilities of MAV autonomous mapping in unknown environments with restricted lighting. Our efficient reconstruction system is capable of generating highly dense point clouds with resolutions ranging from $2 \\mathrm { m m }$ to $8 \\mathrm { m m }$ on surfaces of different complexities at rates approaching $3 0 \\mathrm { H z }$ , fully onboard a MAV. We evaluate the performance of our method on both datasets and real-world platforms and demonstrate its superior accuracy and efficiency compared to existing methods. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Keywords: onboard 3D reconstruction; micro aerial vehicles (MAVs); RGB-D SLAM; fast camera tracking; particle swarm pose optimization; hierarchical sparse 3D representation ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1. Introduction ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "With the rapid development and extensive utilization of micro aerial vehicles (MAVs), MAVs equipped with online 3D reconstruction capabilities have gained considerable attention and importance. By autonomously capturing 3D information about the environment to optimize their action path and behavioral decisions, MAVs can enhance the efficiency and precision of task execution in complex environments. Additionally, they can achieve environment modeling and map construction without human intervention, relieving humans from repetitive or hazardous tasks. Hence, MAVs have the potential to be widely employed in areas such as building maintenance, geological exploration, and emergency rescue. The research on simultaneous localization and dense mapping (Dense SLAM) methods has been advancing with the progression of computing power. Powerful Graphics Processing Units (GPUs) are now widely accessible, enabling vision algorithms to process large quantities of data in real-time through parallel processing. Among various approaches in this field, KinectFusion [1] is one of the most representative techniques that enables the construction of dense 3D scenes in real-time via commodity depth sensors. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "To apply dense SLAM in MAVs and adapt to challenging environments such as lightconstrained settings during emergency rescue missions, two challenges must be addressed. Firstly, it is a challenge to robustly track fast camera motion, which results in significant motion blur in RGB images, particularly in lighting-constrained conditions, and this issuemotion blur in RGB images, particularly in lighting-constrained conditions, and this issue is particularly significant for agile MAVs. Secondly, many existing systems [2–4] are limitedis particularly significant for agile MAVs. Secondly, many existing systems [2–4] are limby computational power, making it difficult to demonstrate their real-time capabilities onited by computational power, making it difficult to demonstrate their real-time capabilithe onboard computer.ties on the onboard co ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "We propose OwlFusion, an end-to-end solution for onboarding dense RGB-D recon-We propose OwlFusion, an end-to-end solution for onboarding dense RGB-D reconstruction of scalable scenes for fast-moving MAVs. Our method relies solely on depthstruction of scalable scenes for fast-moving MAVs. Our method relies solely on depth ininformation as the depth image actively sensed is not limited by lighting. Additionally,formation as the depth image actively sensed is not limited by lighting. Additionally, depth images typically do not produce the same full-frame pixel blur as RGB imagesdepth images typically do not produce the same full-frame pixel blur as RGB images when when the camera is moving fast [4]. To address the challenges of dense SLAM applicationthe camera is moving fast [4]. To address the challenges of dense SLAM application in in fast-moving MAVs, we introduce two key methods. Firstly, we propose a fast posefast-moving MAVs, we introduce two key methods. Firstly, we propose a fast pose estiestimation method that reduces the consumption of many unnecessary particle fitnessmation method that reduces the consumption of many unnecessary particle fitness evaluevaluations by introducing planar constraints. This method accelerates the convergenceations by introducing planar constraints. This method accelerates the convergence effiefficiency of optimized iterations using the same computational resources and enablesciency of optimized iterations using the same computational resources and enables realreal-time tracking of high-speed camera motion on onboard computing devices. Secondly,time tracking of high-speed camera motion on onboard computing devices. Secondly, we we integrate surface hierarchical sparse representation and particle swarm pose optimiza-integrate surface hierarchical sparse representation and particle swarm pose optimization tion methods, and weighted depth fusion of depth images with noise at different voxelmethods, and weighted depth fusion of depth images with noise at different voxel levels levels to achieve real-time, high-quality reconstruction of large-scale scene 3D models onto achieve real-time, high-quality reconstruction of large-scale scene 3D models on an airan airborne computer with very limited computational and storage resources. We mountedborne computer with very limited computational and storage resources. We mounted a a commodity depth camera, Intel RealSense D435i, on a MAV with a wheelbase of onlycommodity depth camera, Intel RealSense D435i, on a MAV with a wheelbase of only 450 $4 5 0 \\mathrm { m m }$ and achieved real-time 3D reconstruction of scalable scenes, as shown in Figure 1. achieved real-time 3D reconstruction of scalable scenes, as shown in Figure 1. ",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/10f47fae628d96ade383cfbcc2e05424088423f14b28ab96793d776f009d7bd8.jpg",
        "img_caption": [
            "Figure 1. MAV performing onboard scalable scan and dense mesh being generated in real time. Our onboard.lightweight system is capable of generating sub-centimeter resolution meshes at $3 0 \\mathrm { H z } ,$ fully onboard. "
        ],
        "img_footnote": [],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "This paper is divided into five sections. We have introduced the research issues and objectives in this section. Section 2 presents an overview of existing methods and their limitations, as well as a discussion of the relevant literature. Section 3 outlines the proposed methodology, with a general overview of the approach and detailed descriptions of individual building blocks in subsections. Section 4 reports on the experimental results, including a description of the setup, presentation of results, and analysis. Section 5 concludes the article with a summary of the findings, implications of the results, limitations of the study, and suggestions for future research. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2. Related Work ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "elated Work With advancements in GPU architecture and General-Purpose Graphics Processing Unit With advancements in GPU architecture and General-Purpose Graphics Processing Unit(GPGPU) algorithms, real-time 3D reconstruction has significantly improved since the (GPGPU) algorithms, real-time 3D reconstruction has significantly improved since the in-inception of DTAM [5]. There has been a substantial increase in the number of publications ception of DTAM [5]. There has been a substantial increase in the number of publications in this field in the past decade. Unlike DTAM, which achieves real-time dense 3D scene in this field in the past decade. Unlike DTAM, which achieves real-time dense 3D scene reconstruction using RGB cameras, we are primarily interested in using low-cost RGB",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "D sensors. These sensors are more readily available and provide more accurate depth information. In this section, we provide an overview of related systems, with a focus on 3D reconstruction methods that rely on RGB-D cameras and tracking methods for fast-moving cameras. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.1. Real-Time RGB-D Reconstruction ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "The representative work for real-time dense 3D reconstruction using RGB-D cameras is KinectFusion [1]. It was the first to demonstrate convincing real-time 3D reconstruction results. Prior to this, there were also some great attempts, a famous example being the method of Curless and Levoy [6], which is based on active triangulation sensors such as laser range scanners and structured light cameras, and can generate very high-quality results. The characteristic of this method is the use of a fully volumetric data structure to implicitly store samples of the continuous function, with the depth map being transformed into a Truncated Signed Distance Function (TSDF), and the accumulated averages becoming a regular voxel grid. Finally, the reconstructed surface is extracted as the zero-level set of the implicit function through ray casting. Unfortunately, due to computing limitations, Curless and Levoy’s work did not achieve the real-time 3D reconstruction of scene objects. KinectFusion inherits and improves upon such methods and achieves small-scale real-time dense 3D reconstruction with the help of high-performance graphics computing units. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "To reconstruct larger spaces, Whelan et al. [2] extended the pipeline of KinectFusion, allowing voxels to flow out of the GPU based on camera motion to make room for storing new data. However, this moving volume process is one-way and lossy, with surface data being compressed into meshes, which cannot flow back to the GPU once it has moved to the CPU. To address this issue, Nießner et al. [7] proposed a bidirectional data exchange method that allows for the flexible exchange of reconstructed surface data between the GPU and CPU, pushing the application of real-time 3D reconstruction to even larger spaces. However, even with the support of the moving volume method and data exchange method, the use of regular voxels is still limited in reconstructing larger scenes. This is because regular voxels densely represent both empty space and surface, namely free space, leading to a lot of wasted graphics memory. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Although tree-based hierarchical data structures [8–10] can effectively subdivide space and avoid graphics memory waste, they are not effectively parallelizable when the computational complexity increases. Point-based methods [11–17], on the other hand, do not require spatial data structures, but the reconstruction quality has trouble matching the results achieved by volume-based reconstruction methods. To address these issues, a new data structure has been proposed by Nießner et al. [7] and Kähler et al. [18], which subdivides space into a group of sparse sub-blocks and uses hash functions to access them efficiently. Building on this data structure, ref. [19] combines the features of tree structures to provide surface representations at different resolutions, which effectively solves the problems of scene expansion and memory occupation in reconstruction. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "One of the challenges of conducting real-time RGB-D-based 3D reconstruction on MAVs is their limited computing and storage resources. Previous vision-based 3D reconstruction systems on drones have achieved success only by transmitting data back to a ground station for processing [20–23], generating only sparse maps for navigation online [24–27], or only being able to reconstruct small-scale 3D maps [28–31] onboard. These systems can operate onboard but are prone to failure during fast drone movement, which is another focus of this article and also another challenge in conducting RGB-D-based real-time 3D reconstruction on drone platforms. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.2. Pose Estimation of Fast Camera Motion ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Although the fast-moving advantages of MAVs have undoubtedly heightened task efficiency, they have also presented substantial challenges in estimating their motion states. Fast camera movement poses two major challenges to camera pose estimation. Firstly, fast camera motion causes significant rotation, rendering the optimization of camera pose highly nonlinear. When seeking to optimize the pose using gradient descent, it is easy for the optimization to become trapped in a local optimum. Secondly, fast camera motion can lead to serious motion blur in RGB images, especially under dark lighting conditions and when taking close-up shots. The motion blur in images makes it difficult to perform reliable RGB feature tracking, which is catastrophic for many feature-based SLAM methods [32,33] and dense RGB-D 3D reconstruction methods [3]. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Numerous researchers have adopted various approaches from different perspectives to address the problem of fast-moving camera pose estimation. At the camera level, a straightforward method to mitigate the adverse effects of image motion blur is to increase the camera’s frame rate. However, high frame rates lead to brief exposure times that lower the image’s signal-to-noise ratio, especially under dim lighting conditions [34,35]. The sudden surge in data volume within a short period significantly increases the computational cost of the system, which is unfavorable for real-time algorithm execution. Saurer et al. [36] have also considered the jelly effect caused by fast-moving rolling-shutter cameras. Another study used event cameras for fast-moving pose estimation [37]. At the image level, researchers have attempted to minimize the negative effects of motion blur on the image to the maximum extent possible by performing image deblurring before feature extraction [38,39]. Unfortunately, considering the computational cost, these methods are challenging to use in real-time 3D reconstruction systems. Introducing additional information, such as fusing Inertial Measuring Unit (IMU) data, is also an effective method for estimating fast-moving camera pose. The IMU provides acceleration data at high frequencies and can serve as good initialization to predict inter-frame motion during gradient descent pose optimization [40]. Considering the high cost of IMUs, some researchers have sought to use low-cost IMU sensors to assist pose estimation [41]. However, the gyroscope sensors of IMUs, especially those built into commodity RGB-D cameras, are more effective in measuring directional changes than estimating translation. Many researchers have found that translation errors are too large to be used for tracking, either serving for attitude initialization [18,42,43] or for joint optimization [44]. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Another unique approach is to utilize the Particle Filter Optimization (PFO) algorithm, which tracks the camera’s rapid movements solely based on depth images. This is because fast camera movements may cause motion blur in RGB images but have a smaller impact on depth images. Fast camera moving often results in depth value overshoot or undershoot at the foreground and background transitions, rather than mixed pixel depth values across the entire image [45]. The basic idea of the PFO algorithm is to transform the objective function into a target Probability Density Function (PDF), and then to simulate the target PDF through sequential importance sampling. It is hoped that the optimal value of the objective function can be covered by the sampled particles. Particle Swarm Optimization (PSO) randomly generates a set of particles and drives them to move towards good local optima based on the designed system dynamic function. Ji et al. [46] use the update and optimization of particle swarm as a system dynamic to drive the movement of the particle ensemble. However, due to the high cost of continuous sampling and updating of particles, it has not been widely adopted in real-time applications. In a recent attempt, the problem of dense particle sampling and updating in standard PFO that affects the real-time performance of the system was solved by moving and updating a pre-sampled Particle Swarm Template (PST) instead of sequential importance sampling [4]. This method shows good real-time performance on a ground workstation, but still cannot run in real-time on airborne computing hardware platforms. We have provided a table comparing the most representative real-time RGB-D reconstruction systems in terms of four key capabilities: fastmoving tracking, sparse representation, scalable reconstruction, and onboard performing, as shown in Table 1. Our proposed method combines the sparsity of surface representation to enhance PFO, aiming to reduce system complexity, accelerate optimization convergence speed, and overcome certain limitations present in existing systems. ",
        "page_idx": 3
    },
    {
        "type": "table",
        "img_path": "images/12e940e6f6d4a4065e830432db922b88e11421daeee1b472c43435696f61eaf3.jpg",
        "table_caption": [
            "Table 1. State-of-the-art real-time dense 3D reconstruction systems based on RGB-D camera.Table 1. State-of-the-art real-time dense 3D reconstruction systems based on RGB-D camera.Tracking sentation construction forming tems. vergence speed, and overcome certain limitations present in existing sys-tation to enhance PFO, aiming to reduce system complexity, accelerate 5 of 20 5 of 20 surface representation to enhance PFO, aiming to reding, as shown in Table 1. Our proposed method combines the sparsity of onboard performing, as shown in Table 1. Our proposed method combines the sparsity oonboard performing, as shown in Table 1. Our proposeTable 1. State-of-the-art real-time dense 3D reconstruction systems based on RGB-D camera.  d $\\checkmark$ notes the presence of the specific capability, whidenotes the presence of the specific capability, whileKinectFusion [1]   of the specific capability, while  denotes the absencnotes the presence of the specific capability, while  notes the presence of the specific cnotes presene-art real-time dense 3D reconstruction systems basTable 1. State-of-the-art real-time dense 3D reconstrug, as shown in Table 1. Our proposed method cTable 1. State-ofvergence speed, and overcome certain limitatiooptimization ctation to enhance PFO, aiming to reduce systesurface representation to enhance PFO, aimingsurface represnotes the presence of the specific capability, while  $\\times$  denotes the absence of the specific capabili denotes the absence of the specific capability.of the specific capability.notes the absence of the specific capability.ability, while  denotes the absence of the sp of the specific capability, while  denotes th on RGB-D camera.  de-ion systems based on RGB-D camera.  de-bines the sparsity of he-art real-time dense 3D reconstruction syst present in existing sys-vergence speed, and overcome certain li complexity, accelerate o reduce system complexity, accelerate 5 of 20 tation to enhance PFO, aiming to reducenotes the absence of the specific capability. "
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>Systems</td><td>Fast-Moving</td><td> Representaion</td><td>Recocalrultion</td><td>Penbrmidg</td></tr><tr><td>KinectFusion [1]</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>Kintinuous [2]</td><td>×</td><td>×</td><td>√</td><td>×</td></tr><tr><td>Voxel Hashing [7]</td><td>×</td><td>√</td><td>√</td><td>×</td></tr><tr><td>InfiniTAM[15]</td><td></td><td>√</td><td>√</td><td>√</td></tr><tr><td>Hierarchical voxels [19]</td><td>×</td><td>√</td><td>√</td><td>×</td></tr><tr><td>BundleFusion [3]</td><td>×</td><td>×</td><td>√</td><td>×</td></tr><tr><td>RoseFusion 4]</td><td>√</td><td>×</td><td>×</td><td>×</td></tr></table></body></html>",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "aptured in real-t RoseFusion [4] ]  els [19]    nectFusion [1]  g  sentatioTrackinFast-MovVoxel Hashing [7]3. Methodology ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": ". Methodology  thodology  3. Methodology 3. Methodology            RoseFusion [4]           ous [2]              9]    rchical voxels [19]       Hierarchical voxels [19]  construction forming TAM [15]    InfiniTAM [15]    InfiniTAM [15]   The input for onboard 3D reconstruction consists of a sequence of RGB-D frames The input for onboard 3D reconstruction  of the captured scene and the 6 Degrees of Free௞ ௞ ௞ ௞ ଷor onboard 3D reconstruction consists of a sequenThe input for onboard 3D reconstruction consThe input for onboard 3D recThe input for  3. Methodology  [19]   3. Methodology              xel Hashing [7]             ]   BundleFusion [3]     BundleFusion [3]  Hierarchical voxels [19]   Hierarchical voxels [19]  Hierarchical voxelscaptured in real-time by RGB-D cameras, denoted by The input for onboard 3D reconstruction con $\\{ I _ { c } , I _ { d } \\} _ { k = 0 : K }$ sequecamer frameuence onsistrecons  where uence $I _ { c }$ e of poseRGof a uctiand f RG $I _ { d }$ GB-D framajectory D frames quence of consists o    representD frames capres?? the globrandoml-time by and depred scencaptureresent t?? of thfor onbol-time by and depThcaptureresent tfor onbol-time byls [19] y for onbols [19] ierarchicundleFusy ls [19] y RoseFu3. Meth BundleRoseFucapturresent ?? of t al-timB andured te syizatiomera resp6 Deme bd dep scenptursent of tconstmera resp onbme bd depconstmeraconst9] undleoseFud thend ded sce B-D cameras, denoted by ሼ??௖, ??ௗሽ௞ୀ଴:௄ wmages, respectively. The output is the sund the 6 Degrees of Freedom (6DoF) cam employ our method, OwlFusion, within hich is the de facto method for large-scaed by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where ??௖ and ??ௗ rep-The output is the surface reconstruction Freedom (6DoF) camera pose trajectory  cameras, denoted by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where es, respectively. The output is the surfacehe 6 Degrees of Freedom (6DoF) camera l-time by RGB-D cameras, denoted by ሼ??and depth images, respectively. The outred scene and the 6 Degrees of Freedom captured in real-time by RGB-D cameraresent the RGB and depth images, respe?? of the captured scene and the 6 Deonsists of a sequence of RGB-D frames ed by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where ??௖ and ??ௗ rep-The output is the surface reconstruction reconstruction consists of a sequence of  cameras, denoted by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where es, respectively. The output is the surface  The input for onboard 3D reconstrcaptured in real-time by RGB-D cameraresent the RGB and depth images, respeonsists of a sequence of RGB-D frames ed by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where ??௖ and ??ௗ rep-    The input for onboard 3D reconstrcaptured in real-time by RGB-D cameraonsists of a sequence of RGB-D frames             3. Methodology        RoseFusion [4]  3. Methodology       3]     BundleFusion [3]  RoseFusion [4]  s of Freedom (6DoF) camera pose trajectory es, respectively. The output is the surfacthe 6 Degrees of Freedom (6DoF) camera $s$ $\\left\\{ \\left[ \\mathbf { R } ^ { k } \\middle | \\mathbf { t } ^ { k } \\right] \\right\\} _ { k = 0 : K } ,$ $\\mathbf { R } ^ { k } \\in \\mathbb { S } \\mathbb { O } _ { 3 }$ $\\mathbf { t } ^ { k } \\in \\mathbb { R } ^ { 3 }$ ௞ୀ଴:௄, where  ଷ and   represent the 3D rotation and translationenge is to estimate the 6DoF pose of each frame and to fuse the captured surface data. here ?? ∈ ????ଷ and ?? ∈ ℝ represent the 3D rotation and translation in ሼሾ?? |?? ሿሽ௞ୀ଴:௄, where ?? ∈ ????ଷ and ?? ∈ ℝ represent the 3D rotation and translation in ሼሾ?? |?? ሿሽ௞ୀ଴:௄, where ?? ∈ ????ଷ and ?? ∈ ℝ represent the 3D rotation anሼሾ?? |?? ሿሽ௞ୀ଴:௄, where ?? ∈ ????ଷ and ?? ∈ ℝ represent thed scene and the 6 Degrees of Freedom (6DoF) camera pose trajectory ?? of the captured scene and the 6 Degrees of Freedom (6DoF) camera pose trajectory ?? of the captured scene and the 6 Degrees of Freedom      seFusion [4]   time by RGB-D cameras, denoted by ሼ?? , ?? ሽ where ?? and ?? rep-     captured in real-time by RGB-D cameras, denoted by ሼ??or onboard 3D reconstruction consists of a sequence of RGB-D frames The input for onboard 3D reconstruction consists of a sequence of RGB-D frames    The input for onboard 3D reconstruction consists o 3. Methodology  3. Methodology 3. Methodology coordinate system. We employ our method, OwlFusion, within a framework of randomized the global coordinate system. We employ our method, OwlFusion, within a frameworkhe randomized optimization framework allows for fast camera motion tracking in low-nate system. We employ our method, OwlFusion, within a framework of he global coordinate system. We employ our method, OwlFusion, within a framework of the global coordinate system. We employ our method, OwlFusion, within the global coordinate system. We employ our method, Owhere ?? ∈ ????ଷ and ?? ∈ ℝ represent the 3D rotation and translation in ሼሾ?? |?? ሿሽ௞ୀ଴:௄, where ?? ∈ ????ଷ and ?? ∈ ℝ represent the 3D rotation and translation in ሼሾ?? |?? ሿሽ௞ୀ଴:௄, where ?? ∈ ????ଷ and ?? ∈ ℝ represent thed scene and the 6 Degrees of Freedom (6DoF) camera pose trajectory  of the captured scene and the 6 Degrees of Freedom      time by RGB-D cameras, denoted by ሼ?? , ?? ሽ where ?? and ?? rep-captured in real-time by RGB-D cameras, denoted by ሼ?? , ?? ሽ where ?? and ?? rep-captured in real-time by RGB-D cameras, denoted by ሼ?? The input for onboard 3D reconstruction consists of a sequence of RGB-D frames The input for onboard 3D reconstruction consists of a sequence ofThe input for onboard 3D reconstruction consists ooptimization [4], which is the de facto method for large-scale high-quality online dense 3D randomized optimization [4], which is the de facto method for large-scale high-quaight conditions. To reduce the computational cost and accelerate the optimization con-mization [4], which is the de facto method for large-scale high-quality randomized optimization [4], which is the de facto method for large-scale high-quality randomized optimization [4], which is the de facto method for large-scrandomized optimization [4], which is the de facto metnate system. We employ our method, OwlFusion, within a framework of he global coordinate system. We employ our method, OwlFusion, within a framework of ሼ?? , ?? ሽ ?? ??the global coordinate system. We employ our method, Ohere  ଷ and   represent the 3D rotation and translation in nstruction consists of a sequence of RGB-D frames onboard 3D reconstruction consists of a sequence of RGB-D frames ௞ୀ଴:௄, where  ଷ and   represent th௞ ௞ ଷMethodology  ??nd depth images, respectively. The output is the surface reconstruction resent the RGB and depth images, respectively. The output is the surface reconstruction resent the RGB and depth images, respectively. The out captured in real-time by RGB-D cameras, denoted by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where ??௖ and ??ௗ rep-captured in real-time by RGB-D cameras, denoted by ሼ??௖, ??ௗሽ௞ୀ଴:௄ wherecaptured in real-time by RGB-D cameras, denoted by ሼ??reconstruction on low-computational hardware platforms. The key challenge is to estimate online dense 3D reconstruction on low-computational hardware platforms. The key chergence of randomized optimization on low-computational hardware, we introduce pla-reconstruction on low-computational hardware platforms. The key chal-online dense 3D reconstruction on low-computational hardware platforms. The key chal-online dense 3D reconstruction on low-computational hardware platformonline dense 3D reconstruction on low-computational hamization [4], which is the de facto method for large-scale high-quality randomized optimization [4], which is the de facto method for large-scale high-quality  depth images, respectively. The output is the surface reconstruction randomized optimization [4], which is the de facto metmeras, denoted by ሼ?? , ?? ሽ where ?? and ?? rep-e by RGB-D cameras, denoted by ሼ?? , ?? ሽ where ?? and ?? rep-ଷ nstruction consists of a sequence of RGB-D frames onboard 3D reconstruction consists of a sequence of RGB-D frames The input for onboard 3D reconstruction consists of a sequence of RGB-D frames ??௞ ∈ ???? ??௞ ∈ ℝଷ ሼሾ??௞|??௞ሿሽ ??௞ ∈ ???? ??௞ ∈ ℝଷed scene and the 6 Degrees of Freedom (6DoF) camera pose trajectory ?? of the captured scene and the 6 Degrees of Freedom (6DoF) camera pose trajectory ?? of the captured scene and the 6 Degrees of Freedom resent the RGB and depth images, respectively. The output is the surface reconstruction resent the RGB and depth images, respectively. The output is the surfaceresent the RGB and depth images, respectively. The outpthe 6DoF pose of each frame and to fuse the captured surface data. The randomized lenge is to estimate the 6DoF pose of each frame and to fuse the captured surface dar constraints based on sparse representation of the scene surface, which is our key con-ate the 6DoF pose of each frame and to fuse the captured surface data. enge is to estimate the 6DoF pose of each frame and to fuse the captured surface data. lenge is to estimate the 6DoF pose of each frame and to fuse the capturelenge is to estimate the 6DoF pose of each frame and to reconstruction on low-computational hardware platforms. The key chal-online dense 3D reconstruction on low-computational hardware platforms. The key chal-online dense 3D reconstruction on low-computational hamization [4], which is the de facto method for large-scale high-quality espectively. The output is the surface reconstruction  depth images, respectively. The output is the surface reconstruction randomized optimization [4], which is the de facto metmeras, denoted by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where ??௖ and ??ௗ rep-e by RGB-D cameras, denoted by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where ??௖ and ??ௗ rep-tured in real-time by RGB-D cameras, denoted by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where ??௖ and ??ௗ rep-nstruction consists of a sequence of RGB-D frames onboard 3D reconstruction consists of a sequence of RGB-D frames here ??௞ ∈ ???? and ??௞ ∈ ℝଷ represent the 3D rotation and translation in ሼሾ??௞|??௞ሿሽ , where ??௞ ∈ ???? and ??௞ ∈ ℝଷ represent the 3D rotation and translation in ሼሾ??௞|??௞ሿሽ , where ??௞ ∈ ???? and ??௞ ∈ ℝଷ represent th?? of the captured scene and the 6 Degrees of Freedom (6DoF) camera pose trajectory ?? of the captured scene and the 6 Degrees of Freedom (6DoF) camera ?? of the captured scene and the 6 Degrees of Freedom optimization framework allows for fast camera motion tracking in low-light conditions. To The randomized optimization framework allows for fast camera motion tracking in lribution. Figure 2 provides a block diagram overview of our method.  optimization framework allows for fast camera motion tracking in low-The randomized optimization framework allows for fast camera motion tracking in low-The randomized optimization framework allows for fast camera motion The randomized optimization framework allows for fast ate the 6DoF pose of each frame and to fuse the captured surface data. enge is to estimate the 6DoF pose of each frame and to fuse the captured surface data. ௞ ௞ ଷ lenge is to estimate the 6DoF pose of each frame and toreconstruction on low-computational hardware platforms. The key chal- Degrees of Freedom (6DoF) camera pose trajectory scene and the 6 Degrees of Freedom (6DoF) camera pose trajectory online dense 3D reconstruction on low-computational haespectively. The output is the surface reconstruction  depth images, respectively. The output is the surface reconstruction ent the RGB and depth images, respectively. The output is the surface reconstruction meras, denoted by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where ??௖ and ??ௗ rep-e by RGB-D cameras, denoted by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where ??௖ and ??ௗ rep-onboard 3D reconstruction consists of a sequence of RGB-D frames ሼሾ??௞|??௞ሿሽ , where ??௞ ∈ ???? and ??௞ ∈ ℝଷ represent the 3D rotation and translation in ሼሾ??௞|??௞ሿሽ , where ??௞ ∈ ???? and ??௞ ∈ ℝଷ represent the 3D rotation anሼሾ??௞|??௞ሿሽ , where ??௞ ∈ ???? and ??௞ ∈ ℝଷ represent threduce the computational cost and accelerate the optimization convergence of randomized light conditions. To reduce the computational cost and accelerate the optimization cTo reduce the computational cost and accelerate the optimization con-ight conditions. To reduce the computational cost and accelerate the optimization con-light conditions. To reduce the computational cost and accelerate the oplight conditions. To reduce the computational cost and a optimization framework allows for fast camera motion tracking in low-The randomized optimization framework allows for fast camera motion tracking in low-The randomized optimization framework allows for fast ate the 6DoF pose of each frame and to fuse the captured surface data.  ??௞ ∈ ℝଷ represent the 3D rotation and translation in re ??௞ ∈ ???? and ??௞ ∈ ℝଷ represent the 3D rotation and translation in lenge is to estimate the 6DoF pose of each frame and to  Degrees of Freedom (6DoF) camera pose trajectory scene and the 6 Degrees of Freedom (6DoF) camera pose trajectory of the captured scene and the 6 Degrees of Freedom (6DoF) camera pose trajectory espectively. The output is the surface reconstruction  depth images, respectively. The output is the surface reconstruction mization [4], which is the de facto method for large-scale high-quality randomized optimization [4], which is the de facto method for large-scale high-quality e by RGB-D cameras, denoted by ሼ??௖, ??ௗሽ௞ୀ଴:௄ where ??௖ and ??ௗ rep-randomized optimization [4], which is the de facto metists of a sequence of RGB-D frames he global coordinate system. We employ our method, OwlFusion, within a framework of the global coordinate system. We employ our method, OwlFusion, within the global coordinate system. We employ our method, Owoptimization on low-computational hardware, we introduce planar constraints based on vergence of randomized optimization on low-computational hardware, we introduce pomized optimization on low-computational hardware, we introduce pla-vergence of randomized optimization on low-computational hardware, we introduce pla-vergence of randomized optimization on low-computational hardware, wvergence of randomized optimization on low-computatio To reduce the computational cost and accelerate the optimization con-light conditions. To reduce the computational cost and accelerate the optimization con-light conditions. To reduce the computational cost and a optimization framework allows for fast camera motion tracking in low-ploy our method, OwlFusion, within a framework of te system. We employ our method, OwlFusion, within a framework of The randomized optimization framework allows for fast  ??௞ ∈ ℝଷ represent the 3D rotation and translation in re ??௞ ∈ ????ଷ and ??௞ ∈ ℝଷ represent the 3D rotation and translation in ௞|??௞ሿሽ௞ୀ଴:௄, where ??௞ ∈ ????ଷ and ??௞ ∈ ℝଷ represent the 3D rotation and translation in 6 Degrees of Freedom (6DoF) camera pose trajectory  scene and the 6 Degrees of Freedom (6DoF) camera pose trajectory reconstruction on low-computational hardware platforms. The key chal-online dense 3D reconstruction on low-computational hardware platforms. The key chal- depth images, respectively. The output is the surface reconstruction online dense 3D reconstruction on low-computational hay ௖ ௗ ௞ୀ଴:௄ where ௖ and ௗ rep-randomized optimization [4], which is the de facto method for large-scale high-quality randomized optimization [4], which is the de facto method for large-scrandomized optimization [4], which is the de facto metsparse representation of the scene surface, which is our key contribution. Figure 2 provides nar constraints based on sparse represased on sparse representation of the scene nar constraints based on sparse representanar constraints based on snar constromized optimization on low-computationavergence of randomized optimization on lovergence To reduce the computational cost and ach is the de facto method for large-scale hiization [4], which is the de facto method light condploy our method, OwlFusion, within a frate system. We employ our method, OwlFu global coordinate system. We employ our  ??௞ ∈ ℝଷ represent the 3D rotation and trare ??௞ ∈ ????ଷ and ??௞ ∈ ℝଷ represent the 3Date the 6DoF pose of each frame and to fuenge is to estimate the 6DoF pose of eachscene and the 6 Degrees of Freedom (6Dlenge is t output is the surface reconstruction online dense 3D reconstruction on low-comonline dense 3D reconstruonline dea block diagram overview of our method. ",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/acb599970fa52a3b26dfbd6359599433808bfa5f84f1a24041a96061a21331d5.jpg",
        "img_caption": [
            "Figure 2. The proposed OwlFusion overview. The purple arrows indicate operations related to scOur method consists of three main parts: surface measurement, surface reconstruc-osed OwlFusion overview. The purple arrows indicate operations related to scene Figure 2. The proposed OwlFusion overview. The purple arrows indicate operations related to scene Figure 2. The proposed OwlFusion overview. The purple arrows indicate operationFigure 2. The proposed OwlFusion overview. The purple arrowsFigure 2. The proposed OwlFusion overview. The purple arrows indicate operations related to sceneFigure 2. The proposed OwlFusion overview. The purple arrows indicate operations related to reconstruction, the blue arrows indicate operations related to pose estimation, and the yellow iion, and pose estimation. In the surface measurement step, we preprocess each depth  blue arrows indicate operations related to pose estimation, and the yellow indi-reconstruction, the blue arrows indicate operations related to pose estimation, and the yellow indi-reconstruction, the blue arrows indicate operations related to pose estimation, anreconstruction, the blue arrows indicate operations related to posed OwlFusion overview. The purple arrows indicate operations related to scene Figure 2. The proposed OwlFusion overview. The purple arrows indicate operations related to scene Figure 2. The proposed OwlFusion overview. The purple arrowsscene reconstruction, the blue arrows indicate operations related to pose estimation, and the yellow cates optional operations. mage frame input by compations. cates optional operations. cates option blue arrows indicate operatioreconstruction, the blue arrowsosed OwlFusion overview. Theindicates optional operations. "
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Our method consists of three main parts: surface measurement, surface reconstrsists of three main parts: surface measurement, surface reconstruc-Our method consists of three main parts: surface measurement, surface reconstruc-Our method consists of three main parts: surface measurement, surfOur method consists of three main parts: surface ms.  cates optional operations.  arrows indicate operations related to pose estimation, and the yellow indi-OwlFusion overview. The purple arrows indicate operations related to scene Figure 2. The proposed OwlFusion overview. The purple arrowsOur method consists of three main parts: surface measurement, surface reconstruc-Our method consists of three main parts: surface measurement, surface reconstruction, tion, and pose estimation. In the surface measurement step, we preprocess each deurface reconstruction step, we adaptively allocate voxel blocks of different resolutions in stimation. In the surface measurement step, we preprocess each depth tion, and pose estimation. In the surface measurement step, we preprocess each depth tion, and pose estimation. In the surface measurement step, we preproction, and pose estimation. In the surface measurement d consists of three main parts: surface measurement, surface reconstruc-Our method consists of three main parts: surface measurement, surface reconstruc-Our method consists of three main parts: surface mrations.  blue arrows indicate operations related to pose estimation, and the yellow indi-reconstruction, the blue arrows indicate operations related to posed OwlFusion overview. The purple arrows indicate operations related to scene Figure 2. The proposed OwlFusion overview. The purple arrows indicate operations related to scene Figure 2. The proposed OwlFusion overview. The purple arrowstion, and pose estimation. In the surface measurement step, we preprocess each depthand pose estimation. In the surface measurement step, we preprocess each depth image image frame input by computing the vertGPU memory for surface representation baseut by computing the vertex map ???? and norimage frame input by computing the vertex mimage frame input by compuimage framestimation. In the surface measurement step, tion, and pose estimation. In the surface med OwlFusion overview. The purple arrows indicatetion, and pod consists of three main parts: surface measurOur merations.  cates optional  blue arrows indicate operations related to pose esreconstruction, the blue arrows indicate operationsreconstructionFigure 2. The proposed OwlFusion overview. The pFigure 2. The proposed OwlFusiFigure 2. The pimage frame input by computing the vertex frame input by computing the vertex map $\\mathcal { V } ^ { k }$ map ???? and normn the correlation bel map ???? and gen ???? and normal mg the vertex map ut by computing t preprocess each drement step, we perations related to scestimation. In the ent, surface reconsd consists of three rations. ation, and the yellowated to pose estimatie blue arrows indicatle arrows indicate opverview. The purpleosed OwlFusion ovep ?? and normal and normal map $\\mathcal { N } ^ { k }$ map ???? and generen the measured -???? and generat-and normal map ??vertex map ???? andh ocess each depth ace measurement s-n parts: surface mi-nd the yellow indi-erations related to poons related to scene ws indicate operationw. The purple arrows ?? and generat-and generating a ing a Partition Normal M????ut by computing the vertex mage frame input by compue arrows indicate operations rstimation. In the surface meiew. The purple arrows indicatd OwlFusion overview. The pu consists of three main partations. cates optional operations. reconstruction, the blue arrows reconstructioing a Partition Normal Map (Partition Normal Map (PNM) $\\mathcal { P } ^ { k } .$ (PNM) ???? , which we use to introduce plane constraints. In ???? ???? ????p ???? and normal map ???? and generat-g the vertex map ???? and normal map ???? and generat-ed to pose estimation, and the yellow indi-image frame input by computing the vertex map ???? anurement step, we preprocess each depth erations related to scene  arrows indicate operations related to scene tion, and pose estimation. In the surface measurement urface measurement, surface reconstruc-cates optional operations. icate operations related to pose estimation, and the yellow indi-he blue arrows indicate operations related to pose estimation, anreconstruction, the blue arrows indicate operations related to pM) ?? , which we use to introduce plane constraints. In the, which we use to introduce plane constraints. In the surface surface reconstruction step, we adaptively allocate voxel blocks of different resolutionrmal Map (PNM) ???? , which we use to introduce plane constraints. In the ng a Partition Normal Map (PNM) ???? , which we use to introduce plane constraints. In the ons.  ing a Partition Normal Map (PNM) ???? , which we use to intut by computing the vertex map ???? and normal map ???? and generat-operations related to pose estimation, and the yellow indi-ue arrows indicate operations related to pose estimation, and the yellow indi-image frame input by computing the vertex map ???? anstimation. In the surface measurement step, we preprocess each depth iew. The purple arrows indicate operations related to scene d OwlFusion overview. The purple arrows indicate operations related to scene ure 2. The proposed OwlFusion overview. The purple arrows indicate operations related to scene  consists of three main parts: surface measurement, surface reconstruc-Our method consists of three main parts: surface m  surface reconstruction step, we adaptively allocate voxel blocks of different resolutions inreconstruction step, we adaptively allocate voxel blocks of different resolutions in GPU GPU memory for surface representation based on the correlation between the measuction step, we adaptively allocate voxel blocks of different resolutions in surface reconstruction step, we adaptively allocate voxel blocks of different resolutions in surface reconstruction step, we adaptively allocate voxel rmal Map (PNM) ???? , which we use to introduce plane constraints. In the ons.  ing a Partition Normal Map (PNM) ???? , which we use to intut by computing the vertex map ?? and normal map ?? and generat- operations related to pose estimation, and the yellow indi-ue arrows indicate operations related to pose estimation, and the yellow indi-onstruction, the blue arrows indicate operations related to pose estimation, and the yellow indi-stimation. In the surface measurement step, we preprocess each depth  tion, and pose estimation. In the surface measurement GPU memory for surface representation based on the correlation between the measuredmemory for surface representation based on the correlation between the measured depth r surfacGPU monsists ction strmal Mut by crows invalues $\\dot { I } _ { d } ^ { k } ( { \\mathbf { u } } )$ resentation based on the correlation between the measured y for surface representation based on the correlation between the measured ee main parts: surface measurement, surface reconstruc-GPU memory for surface representation based on the coe adaptively allocate voxel blocks of different resolutions in surface reconstruction step, we adaptively allocate voxel M) ?? , which we use to introduce plane constraints. In the ting the vertex map ?? and normal map ?? and generat-image frame input by computing the vertex map ?? an?? ?? ?? ?? ??operations related to scene and the vertex normals, achieving sparse representation of the scene. Based mation. In the surface measurement step??r surface representation based on the corGPU mection step, we adaptively allocate voxel bmain parts: surface measurement, surfaceonsists of three main parts: surface measOur method consists of three main parrmal Map (PNM) ?? , which we use to inting a Parut by computing the vertex map  and??image frame input by computing the ver??ons.  image fr to pose estimation, and the yellow indi-on the allocated voxel space and the pose $\\mathbf { T } ^ { k ^ { * } }$ e preprocess each depth ??tion between the measured ry for surface representation based on the cos of different resolutions in onstruc-ment, surface reconstruc-urface measurement, surface reconstruc-uce plane constraints. In the n Normal Map (PNM) ?? , which we use to intrmal map  and generat-map  and normal map  and generat- input by computing the vertex map  an?? ???? ???????? ??????of the depth images, we continuously weight by computing the vertex map ?? and normal map ?? and generat-?? ?? ?? ??r surface representation based on the correlation between the measured ction step, we adaptively allocate voxel blocks of different resolutions in ain parts: surface measurement, surface reconstruc-onsists of three main parts: surface measurement, surface reconstruc-surface reconstruction step, we adaptively allocate voxrmal Map (PNM) , which we use to introduce plane constraints. In the ng a Partition Normal Map (PNM) , which we use to introduce plane constraints. In thing a Partition Normal Map (PNM) , which we use to ?? ?? ??and fuse the measured depth frames into the volume to reconstruct the scene surface $s$ al Map (PNM) ?? , which we use to introduce plane constraints. In the ?? ???? ???? ???? ??r surface representation based on the correlation between the measured urface measurement step, we preprocess each depth mation. In the surface measurement step, we preprocess each depth GPU memory for surface representation based on the corction step, we adaptively allocate voxel blocks of different resolutions in surface reconstruction step, we adaptively allocate voxel blocks of different resolutions in onsists of three main parts: surface measurement, surface reconstruc-surface reconstruction step, we adaptively allocate voxel bThe quality of the scene reconstruction heavily relies on the accuracy of the pose estimation. ion step, we adaptively allocate voxel blocks of different resolutions in , which we use to introduce plane constraints. In the al Map (PNM) , which we use to introduce plane constraints. In the ?? ?? ??e vertex map ???? and normal map ???? and generat-by computing the vertex map ???? and normal map ???? and generat-r surface representation based on the correlation between the measuredGPU memory for surface representation based on the correlation betwemation. In the surface measurement step, we preprocess each depth GPU memory for surface representatioce measurement, surface reconstruc-In the pose estimation step, we evaluate the fitness of random particles $P ^ { k ( i ) }$ e measured sed on the cobased on the reconstructed sparse surface $s$ and $\\mathrm { P N M } \\mathcal { P } ^ { k }$ , which serves as a constraint to alleviate the computational burden and accelerate the optimal pose estimation speed. Finally, we use the classical ray-casting method [1] to extract the scene surface. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "3.1. Surface Measurement ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Surface Measurement is the first step in our method, which takes the raw depth image $I _ { d }$ as input. We use $\\mathbf { u } = \\left( x , y \\right) ^ { T } \\in \\mathbb { R } ^ { 2 }$ to represent a two-dimensional pixel on $I _ { d }$ . Given the camera intrinsic parameter matrix $\\mathbf { K } ,$ we convert each depth measurement $I _ { d } ( \\mathbf { u } )$ into the three-dimensional position ${ \\pmb v } ( { \\pmb u } )$ of a vertex in the camera coordinate system using ",
        "page_idx": 5
    },
    {
        "type": "equation",
        "img_path": "images/67cbd4cee344c6c646bb302def247583e9edbd008a6a64e2eadb48d3e5544dac.jpg",
        "text": "$$\n\\boldsymbol { v } ( \\mathbf { u } ) = I _ { d } ( \\mathbf { u } ) \\mathbf { K } ^ { - 1 } \\big ( \\mathbf { u } ^ { \\mathrm { T } } , 1 \\big ) ^ { \\mathrm { T } } \\in \\mathbb { R } ^ { 3 } ,\n$$",
        "text_format": "latex",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "which forms the vertex map $\\mathcal { V } ^ { k }$ corresponding to the depth map. We then determine the normal map $\\mathcal { N } ^ { k }$ at each vertex in the vertex map by computing ",
        "page_idx": 5
    },
    {
        "type": "equation",
        "img_path": "images/a2ce0c7a951f4eaf282522de19fe9d646a1a845b701dce36555f4ff2f648da4d.jpg",
        "text": "$$\nn ( \\mathbf { u } ) = { \\pmb v } _ { h o r } \\times { \\pmb v } _ { v e r } ,\n$$",
        "text_format": "latex",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "where ",
        "page_idx": 5
    },
    {
        "type": "equation",
        "img_path": "images/0bc801e1afd04b18098d30c90a12f9c9fca6f320588dcfda896511854e29927f.jpg",
        "text": "$$\n\\pmb { v } _ { h o r } = ( v ( x - 1 , y ) - v ( x + 1 , y ) ) \\in \\mathbb { R } ^ { 3 }\n$$",
        "text_format": "latex",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "and ",
        "page_idx": 5
    },
    {
        "type": "equation",
        "img_path": "images/0f850092d1aa54c1b0e2a7ceba30d0a64311bfb2e4fd7567f9879acb69d6db06.jpg",
        "text": "$$\n\\boldsymbol { \\mathscr { v } } _ { v e r } = \\left( v ( x , y - 1 ) - v ( x , y + 1 ) \\right) \\in \\mathbb { R } ^ { 3 }\n$$",
        "text_format": "latex",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "represent the direction vector of the three-dimensional points on both sides of the point $\\mathbf { u }$ horizontally and the direction vector of the three-dimensional points on both sides of the point u vertically, respectively. We normalize the cross-product result to obtain the normal vector $n ( \\mathbf { u } )$ at the current point. At this point, the direction of the normal vector points away from the camera center, so we flip the direction to point towards the camera center. Points located at the edge of the depth image are not used to calculate the normal vector. Here, $v ( { \\mathbf u } )$ and $n ( \\mathbf { u } )$ represent the elements in the vertex map $\\mathcal { V } ^ { k }$ and normal map ${ \\mathcal { N } } ^ { k }$ , respectively. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "In OwlFusion, a PNM $\\mathcal { P } ^ { k }$ is generated based on the normal map $\\mathcal { N } ^ { k }$ to introduce planar constraints, as shown in Figure 3. The PNM can be considered as the result of grouping pixels in the normal map $\\mathcal { N } ^ { k }$ based on their similarity in the normal direction. It clusters adjacent pixels based on their similarity in the normal direction and visualizes the clustering result as different-colored regions, each representing a plane. In this process, we use a growth method to obtain the PNM $\\mathcal { P } ^ { k }$ . Specifically, we choose a pixel in the normal map as the seed pixel and calculate the normal angle and Manhattan distance between it and its neighboring pixels. If both evaluation criteria are below a given threshold, the neighboring pixel is accepted as a region growth unit. We choose the normal angle threshold as $0 . 6 ^ { \\circ }$ and the Manhattan distance threshold as the sum of the image’s width plus height based on experience. After growth, if the number of pixels in a segmented region is less than $4 \\%$ of the total image pixels, the region is rejected; otherwise, the segmented region is too small. In a region growth process, the seed pixel and the pixels in the segmented region will not be repeatedly calculated. Through the segmented normal map, we can quickly drive the particle swarm to move to the vicinity of the optimal pose, thereby accelerating the optimization convergence. Compared to directly extracting the scene plane on the depth map, the normal map-based region growth method is more robust to depth map measurement noise and faster in parallel computing on graphics processing units. ",
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/b63525e92b5459ae383850700b11f8af336412e91a2af8342d9d4c892e65aad5.jpg",
        "img_caption": [
            "gure 3. The PNMs generated for two different viewpoints. The top row displays the frameFigure 3. The PNMs generated for two different viewpoints. The top row displays the frames red with slow camera motion, while the bottom row displays the frames captured with fastcaptured with slow camera motion, while the bottom row displays the frames captured with fast a motion, where noticeable motion blur can be observed in the RGB images.camera motion, where noticeable motion blur can be observed in the RGB images. "
        ],
        "img_footnote": [],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "3.2. Surface Reconstruction ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The design requirements for adaptive resolution fast surface reconstruction include The design requirements for adaptive resolution fast surface reconstruction inefficient hierarchical voxel allocation and noise-robust depth data fusion methods. To fficient hierarchical voxel allocation and noise-robust depth data fusion methodachieve this, we build upon previous work that uses a fixed number of L-level resolution hieve this, we build upon previous work that uses a fixed number of L-level resollayers to store surface voxels [19] and implement effective access to each level’s voxels yers to store surface voxels [19] and implement effective access to each level’s vusing a hash table [7,18,19]. However, instead of uniform voxel allocation, we selectively ing a hash table [7,18,19]. However, instead of uniform voxel allocation, we selecallocate voxel blocks to pixels in different regions using a PNM as a mask. For pixels in planar regions, we allocate voxel memory at the coarsest resolution level, while for non-planar regions, we skip the coarsest level and directly allocate voxel memory at the next coarsest level. On the coarsest level hash table entry, we implicitly refer to the position of the sub-blocks in finer resolution levels by a special marker. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "st level. On the coarsest level hash table entry, we implicitly refer to the positTo reconstruct the surface, we first dynamically allocate memory for voxels within the e sub-blocks in finer resolution levels by a special marker. camera field of view and then use voxel splitting and merging to achieve a hierarchical To reconstruct the surface, we first dynamically arepresentation of the surface. When a new depth frame $k$ ocate memory for voxels wis input into our system, we e camera field construct a ray $\\mathbfcal { R }$ view and then use voxel splifor each depth measurement $I _ { d } ( \\mathbf { u } )$ and merging to achieve a hierarc with respect to the camera center, presenwhere ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "img_path": "images/c2e7bb523038af7d831a79b674a8d1bbb12be29d726aa70eb53241031e6fbc09.jpg",
        "text": "$$\n\\mathscr { R } = \\mathbf { T } _ { g , k - 1 } \\mathbf { K } ^ { - 1 } \\Bigl ( \\mathbf { u } ^ { \\mathrm { T } } , 1 \\Bigr ) ^ { \\mathrm { T } } .\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Here, $\\mathbf { T } _ { g , k - 1 } = \\left[ \\begin{array} { c c } { \\mathbf { R } _ { g , k - 1 } } & { \\mathbf { t } _ { g , k - 1 } } \\\\ { \\mathbf { 0 } ^ { T } } & { 1 } \\end{array} \\right] \\in \\mathbb { S } \\mathbb { E } _ { 3 }$ is the transformation from frame $k$ in the camera coordinate system to the global frame $g .$ , where $\\mathbf { R } _ { g , k - 1 } \\in \\mathrm { S @ } _ { 3 }$ is the rotation Here, ?? = ൤ ௚,௞ିଵ transformation matrix and $\\mathbf { t } _ { g , k - 1 } \\in \\mathbb { R } ^ { 3 }$ ?? is the transformation from frame ?? iis the translation transformation vector of the ?? 1 previous frame. Given the truncation range $\\mu$ of the TSDF [6], we create a line segment mera coordinate systemalong the direction of ray $\\mathbfcal { R }$ the global frame  , wherwithin the truncation band of $I _ { d } ( { \\mathbf { u } } ) - \\mu$ to $I _ { d } ( \\mathbf { u } ) + \\mu$ e rot. For ansformation matrix and ??௚,௞ିଵ ∈ ℝ is the translation transformation vector of thvoxels that intersect with this line segment, we create a corresponding entry in the hash ous frame. Given the truncation range ?? of the TSDF [6], we create a line segment table and allocate memory for the unallocated voxel blocks on the GPU [7,18,19]. After e direction of ray ?? within the truncation band of ??ௗ(??) − ??voxel memory allocation is complete, we compute the roughness $r ( b )$ ??ௗ(??) + ??. For v of the surface on which the voxels reside, which serves as the criterion for voxel splitting and merging. We define the roughness of the surface using the correlation of normals, calculated as ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 7
    },
    {
        "type": "equation",
        "img_path": "images/03cd9c00cdef06597e9168277cb161296be09b3a921397845b8d0551eb418be8.jpg",
        "text": "$$\nr ( b ) = \\frac { 1 } { m - 1 } \\textstyle \\sum _ { i = 1 } ^ { m } \\bigl ( \\Delta F _ { m } ( i ) - \\overline { { \\Delta F _ { m } } } \\bigr ) ^ { 2 } ,\n$$",
        "text_format": "latex",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "where $\\varDelta F _ { m }$ represents the part of the voxel block $b$ that stores TSDF and $m$ is the number of TSDF values. $\\varDelta$ is the gradient operator, $\\varDelta F _ { m } ( i )$ represents the normal at voxel $i ,$ and $\\overline { { \\varDelta F _ { m } } }$ represents the average normal of $m$ voxels. It should be noted that we use the gradient of TSDF values to calculate voxel normals, rather than directly using normals from the normal map derived from depth measurements. This is because the normal map from depth measurements contains sensor noise, and the TSDF values in voxels are the results of multiple weighted fusion observations with high credibility. When the roughness is greater than the segmentation threshold $t _ { f } ,$ , the current level voxel is segmented. When the roughness is less than the merging threshold $t _ { h } ,$ the current level voxel is merged, and the resolution level of each voxel is recorded. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "After allocating all voxel blocks within the truncation region, the current depth frame is fused with the reconstructed visible surface voxels. To efficiently perform the fusion of TSDF, we first access all entries in the hash table before fusing the depth frames, selecting the hash entries that point to visible voxel blocks within the camera view frustum [7], thereby avoiding empty voxel blocks in the hash table. These hash entries are then processed in parallel to update the TSDF values. The global fusion of all depth maps in the volume is formed as the weighted average of all individually calculated TSDFs from each depth frame, which can be viewed as denoising the global TSDF from multiple noisy TSDF measurements. We adopt the TSDF fusion framework of [18], but redefine the weight $W _ { k }$ of the TSDF. Considering the effect of sensor noise, we define ",
        "page_idx": 7
    },
    {
        "type": "equation",
        "img_path": "images/0f05f5cfeb00fffaf53cacaee8d42e07873b9bbeea5d4beb4271d7f8d6dde9ab.jpg",
        "text": "$$\nW _ { k } ( v ( { \\mathbf { u } } ) ) = \\exp \\left( \\frac { - \\gamma ^ { 2 } { \\cos \\theta } } { 2 \\delta ^ { 2 } I _ { d } ( u ) } \\right) ,\n$$",
        "text_format": "latex",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "where $\\gamma$ is the normalized radial distance of the current depth measurement $I _ { d } ( \\mathbf { u } )$ from the camera center, and $\\delta = 0 . 6$ is derived empirically. We define an imaging validity factor and a scan validity factor, respectively, ",
        "page_idx": 7
    },
    {
        "type": "equation",
        "img_path": "images/f2303f641b529281dcf7219f5adf57c02a6bf1675fd35b55a150e8e3d813e978.jpg",
        "text": "$$\n\\alpha = - \\gamma ^ { 2 } / 2 \\delta ^ { 2 }\n$$",
        "text_format": "latex",
        "page_idx": 7
    },
    {
        "type": "equation",
        "img_path": "images/f8878927f95d13ac1c5aa37c4f591c974aec34bcf767427c43d397417dbbd829.jpg",
        "text": "$$\n\\beta = \\cos { \\theta } / I _ { d } ( \\mathbf { u } ) ,\n$$",
        "text_format": "latex",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "where $\\theta$ is the angle between the ray direction of the depth pixel u and the normal measurement of the corresponding surface point $v ( { \\mathbf { u } } )$ in the local frame. If the depth measurement is within the valid distance range and the scan angle of the visible surface points in the depth map is $0 ^ { \\circ }$ , the validity of the point is maximum $\\beta = 1 .$ , which decreases as the scan distance exceeds the valid range or deviates from $0 ^ { \\circ }$ . As the reconstructed surface may extend beyond or revisit the camera view frustum during system running, a bidirectional GPU–Host data stream scheme [7,18] is used to store the reconstructed voxels that exceed the current camera view frustum in the host, allowing the system to fully utilize the limited GPU memory and performance and enable unlimited reconstruction. When the camera returns to a previously reconstructed position, the voxels stored in the host in that region are streamed back to the GPU for fusion and reuse. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "3.3. Pose Estimation ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Accurate image pose estimation is crucial for surface reconstruction using depth images. However, traditional methods [3,14,33,47–51] for pose estimation on MAV platforms can fail to track the camera due to fast camera motion, resulting in incorrect pose estimation results. To address this challenge, our work uses a particle swarm template random optimization [4]. Unlike previous work, we introduce planar constraints with the help of PNM (Section 3.1) based on hierarchical sparse surface representation (Section 3.2). Our method performs Candidate Particle Set (CPS) filtering firstly before particle fitness evaluation, and subsequent iterations of particle optimization select particles from the CPS. Compared to the Advantage Particle Set (APS) defined in [4], the CPS is a much smaller particle swarm set that is strictly constrained, which helps to reduce computational cost during random optimization and accelerate the convergence of pose optimization iterations. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "To reflect the alignment between the current and the previous frame accurately, we need to determine the overlapping area between the planar regions of the two frames. To identify the set of overlapping pixels $O ^ { k }$ between the PNMs $\\bar { \\mathcal { P } } ^ { k }$ and $\\mathcal { P } ^ { k - 1 }$ , we adopt an unproject-and-reproject approach: ",
        "page_idx": 8
    },
    {
        "type": "equation",
        "img_path": "images/7341bbe4c4c2ec57179f4109d4a590e3a1fef8a25b0d0dd656224a74bb9ffab6.jpg",
        "text": "$$\n\\begin{array} { r } { O ^ { k } = \\bigg \\{ ( i , j ) \\bigg | \\mathbf { T } ^ { k - 1 } \\Big ( \\mathbf { T } ^ { k } \\Big ) ^ { - 1 } [ ( i , j ) ] \\in \\mathcal { P } ^ { k - 1 } w i t h ( i , j ) \\in \\mathcal { P } ^ { k } \\bigg \\} , } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "where $\\mathbf { T } ^ { k }$ is the projection matrix of frame $k$ under the camera pose $P ^ { k }$ . Based on $O ^ { k }$ , CPS is calculated by projecting the overlapping pixels onto the volume and normalizing based on whether the corresponding voxel is in the coarsest voxel level: ",
        "page_idx": 8
    },
    {
        "type": "equation",
        "img_path": "images/c6404f66f2d5649569532b00af1880d45ba6573936b8fef9d4d342d678b55871.jpg",
        "text": "$$\n\\Omega _ { c } ^ { k } = \\Bigg \\{ P ^ { k ( i ) } \\in \\Omega \\Bigg | \\frac { N _ { l } } { \\big | O ^ { k } \\big | } > 0 . 9 6 \\Bigg \\} ,\n$$",
        "text_format": "latex",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "where $P ^ { k ( i ) }$ represents any pose particle in the PTM $\\Omega$ and $N _ { l }$ is the number of overlapping pixels projected onto the coarsest voxel level. Due to the uncertainty of edge pixels in the PNM plane segmentation, we relax the percentage of $N _ { l }$ relative to the total overlapping pixels to $9 6 \\%$ based on the empirical values obtained in the experiment to ensure coverage of the optimal solution. Meanwhile, we use the same method to identify the set of overlapping pixels $Q ^ { k }$ between the depth frames $I _ { d } ^ { k }$ and $I _ { d } ^ { k - 1 }$ : ",
        "page_idx": 8
    },
    {
        "type": "equation",
        "img_path": "images/94f9bf0bdddea9743a7aa7f0c91ccefe5894a0e6a37b01bd3a7df76ea48f2a4e.jpg",
        "text": "$$\n\\begin{array} { r } { Q ^ { k } = \\bigg \\{ ( i , j ) \\bigg | \\mathbf { T } ^ { k - 1 } \\Big ( \\mathbf { T } ^ { k } \\Big ) ^ { - 1 } [ ( i , j ) ] \\in I _ { d } ^ { k - 1 } w i t h ( i , j ) \\in I _ { d } ^ { k } \\bigg \\} . } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "In each iteration $t$ during the optimization of $P ^ { k }$ , $Q _ { t } ^ { k }$ is used as the valid pixel set to evaluate the particle fitness: ",
        "page_idx": 8
    },
    {
        "type": "equation",
        "img_path": "images/36f3fef7be3a631c8fe1e6d753eeb708ee724b5fab3dae12f2bcae130a5477b6.jpg",
        "text": "$$\n\\rho \\left( P _ { t } ^ { k ( i ) } \\right) = \\exp \\left( - \\frac { \\sum _ { ( i , j ) \\in Q _ { t } ^ { k } } \\psi \\left( \\mathbf { R } _ { t } ^ { k } \\mathbf { x } _ { i j } + \\mathbf { t } _ { t } ^ { k } \\right) } { \\left| Q _ { t } ^ { k } \\right| } \\right) ,\n$$",
        "text_format": "latex",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "where $\\mathbf { R } _ { t } ^ { k }$ and $\\mathbf { t } _ { t } ^ { k }$ represent the rotation and translation of the pose particle $P _ { t } ^ { k ( i ) } \\in \\Omega _ { c } ^ { k } ,$ and $\\psi ( \\cdot )$ represents the so far constructed TSDF. Note that the inter-frame overlap is deliberately maintained at a non-negligible level to avoid any potential over-evaluation of poses with minimal overlap. Our PST scaling scheme ensures that the sampled transformation, relative to the pose of the previous step, remains within a controlled range of $1 0 \\mathrm { c m }$ in translation and $1 0 ^ { \\circ }$ in rotation. This careful constraint ensures that the evaluations remain within appropriate bounds and accurately reflect the relevant transformations. In each iteration, we use the method in [4] to scale and move PST until we find the optimal pose $\\mathbf { T } ^ { k ^ { * } }$ , i.e., ",
        "page_idx": 8
    },
    {
        "type": "equation",
        "img_path": "images/7842618a79b3c39e800a6eddcd4dc774f9a5ac85baa3e27cb99cb88064d06442.jpg",
        "text": "$$\n\\begin{array} { r } { \\mathbf { T } ^ { k ^ { * } } = \\Big ( P _ { t } ^ { k ( i ) } \\Big | \\operatorname* { m a x } \\Big ( \\rho \\Big ( P _ { t } ^ { k ( i ) } \\Big ) \\Big ) \\Big ) . } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "The experiments will demonstrate that our method requires significantly fewer iterations to find the optimal solution compared to the method in [4] while also reducing the time and resource consumption for pose estimation. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "4. Experiments ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "In this section, we first introduce the hardware information and computational performance of our platform, as well as the settings of experimental parameters. Then, we describe the dataset used in our experiments. After that, we evaluate the effectiveness of planar constraints in improving the efficiency of random optimization for pose estimation and the importance of imaging validity factor and scan validity factor weighting for surface reconstruction, as well as the memory usage and processing efficiency of our entire system. Finally, we compare our onboard real-time 3D reconstruction method with state-of-the-art methods through qualitative and quantitative experiments. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "4.1. Performance and Parameters ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "We conducted all experiments on an embedded computing device, the Nvidia Xavier NX. This device is equipped with a six-core ARM Cortex-A57 CPU and a dual-core NVIDIA Denver $2 . 0 \\mathrm { C P U }$ , along with 8 GB of LPDDR4x RAM and an NVIDIA Volta GPU with 512 CUDA cores, providing a processing capacity of 1.3 TFLOPS. Furthermore, it is equipped with 16 GB of high-speed HBM2 memory, which offers fast data transfer and processing capabilities. We set the basic voxel size to $b _ { 0 } = 2 { \\mathrm { m m } } ,$ which provides a very high level of detail. In our hierarchical representation, we used three levels of resolution, resulting in a coarsest voxel size of $b _ { 2 } = 8 \\mathrm { m m }$ . The truncation distance for the TSDF was set to $\\mu = 2 4 \\mathrm { m m }$ . To increase the credibility of our experiments, we did not add any additional acceleration processing to the proposed method, kept the consistency of the experimental data input, and left all other parameters at their default values unless otherwise specified. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "4.2. Benchmark ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "We defined a fast camera motion as having a linear velocity greater than $1 \\mathrm { m } / \\mathrm { s }$ or an angular velocity greater than $2 \\mathrm { r a d } / \\mathrm { s }$ . We found two publicly available datasets, FastCaMo [4] and FMDataset [52], that satisfy this definition. FastCaMo is the first RGBD sequence dataset specifically designed for fast camera motion, comprising synthetic (FastCaMo-Synth) and real captured (FastCaMo-Real) parts, as well as ground truth trajectories and reconstructions for evaluating system performance. FMDataset not only provides color and depth images captured by a depth camera but also IMU information of the camera. We also evaluated our method on traditional RGB-D datasets, including TUM RGB-D [53], ICL-NUIM [54], and ETH3D [55]. The motion speed of the former two is lower than our defined fast camera motion (usually less than $1 \\mathrm { m } / \\mathrm { s }$ ), while in ETH3D, we focus on three sequences with the prefix “camera shake”, all of which have angular velocities above $2 . 5 \\mathrm { r a d } / \\mathrm { s }$ . In addition, we captured four RGB-D sequences in our flight experiments, namely, “corridor1_slow”, “corridor1_fast”, “corridor2”, and “courtyard”. Table 2 provides speed information for all datasets being tested. Note that the speed of the publicly available datasets comes from ground truth camera trajectories obtained by a visual motion capture system, while the speed of FastCaMo-Real, FMDataset, and our captured real-world RGB-D sequences comes from successfully tracked camera trajectories, as it is difficult for a visual motion capture system to track such fast camera motion. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "4.3. Evaluation ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "In this section, we conducted comprehensive ablation experiments to evaluate key design aspects of our system. This included assessing the effectiveness and efficiency of the planar constraint in the random optimization process, evaluating the benefits of a hierarchical sparse surface representation for scalable reconstruction, and analyzing the weighted depth fusion scheme incorporating imaging and scan validity. Additionally, we assessed the efficiency of the overall system. These experiments provide valuable insights into the performance and significance of each component in our proposed method. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 10
    },
    {
        "type": "table",
        "img_path": "images/64c273a9eca2dbb7466b469993d4f2b17c73406249601615fd5c5f428076dc8b.jpg",
        "table_caption": [
            "Table 2. Statistics on camera moving speed (average linear velocity $\\overline { { v } } _ { \\iota }$ , maximum linear velocity $v _ { m a x } ,$ average angular velocity $\\overline { { \\omega } }$ , and maximum angular velocity $\\omega _ { m a x . }$ ) for different benchmark datasets. "
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>Sequence</td><td> (m/s)</td><td>Umax(m/s)</td><td>W (rad/s)</td><td>Wmax(rad/s)</td></tr><tr><td>TUM_fr1/desk</td><td>0.41</td><td>0.66</td><td>0.41</td><td>0.94</td></tr><tr><td>TUM_fr1/room</td><td>0.33</td><td>0.76</td><td>0.52</td><td>0.85</td></tr><tr><td>TUM_fr3/office</td><td>0.25</td><td>0.36</td><td>0.18</td><td>0.35</td></tr><tr><td>ICL_lr_kt0</td><td>0.13</td><td>0.27</td><td>0.16</td><td>0.33</td></tr><tr><td>ICL_lr_kt1</td><td>0.05</td><td>0.09</td><td>0.10</td><td>0.40</td></tr><tr><td>ICL_lr_kt2</td><td>0.28</td><td>0.40</td><td>0.23</td><td>0.46</td></tr><tr><td>ICL_lr_kt3</td><td>0.27</td><td>0.38</td><td>0.12</td><td>0.41</td></tr><tr><td>ETH3D_camera_shakel</td><td>0.46</td><td>0.64</td><td>1.88</td><td>2.65</td></tr><tr><td>ETH3D_camera_shake2</td><td>0.33</td><td>0.48</td><td>1.90</td><td>3.27</td></tr><tr><td>ETH3D_camera_shake3</td><td>0.37</td><td>0.51</td><td>2.16</td><td>3.43</td></tr><tr><td>FastCaMo_real/lab</td><td>0.98</td><td>3.62</td><td>0.91</td><td>5.20</td></tr><tr><td>FastCaMo_real/apartmentl</td><td>1.05</td><td>4.22</td><td>1.08</td><td>5.73</td></tr><tr><td>FastCaMo_real/apartment2</td><td>1.71</td><td>3.73</td><td>1.38</td><td>4.21</td></tr><tr><td>FastCaMo_synth/apartmentl</td><td>1.53</td><td>3.88</td><td>0.92</td><td>2.08</td></tr><tr><td>FastCaMo_synth/hotel</td><td>1.66</td><td>3.94</td><td>1.13</td><td>2.23</td></tr><tr><td>FMDataset_dorm1_fast1</td><td>0.52</td><td>0.92</td><td>1.24</td><td>2.59</td></tr><tr><td>FMDataset_dorm2_fast</td><td>0.75</td><td>1.60</td><td>1.23</td><td>2.16</td></tr><tr><td>FMDataset_hotel_fast1</td><td>0.75</td><td>1.26</td><td>1.29</td><td>2.34</td></tr><tr><td>FMDataset_livingroom_fast</td><td>0.53</td><td>1.77</td><td>0.85</td><td>2.41</td></tr><tr><td>FMDataset_rent2_fast</td><td>0.83</td><td>1.54</td><td>1.31</td><td>2.27</td></tr><tr><td>Ours_corridor1_slow</td><td>0.40</td><td>0.73</td><td>0.55</td><td>0.89</td></tr><tr><td>Ours_corridorl_fast</td><td>0.92</td><td>1.44</td><td>1.31</td><td>3.03</td></tr><tr><td>Ours_corridor2</td><td>0.87</td><td>1.95</td><td>1.42</td><td>2.89</td></tr><tr><td>Ours_courtyard</td><td>1.01</td><td>2.30</td><td>1.09</td><td>3.37</td></tr></table></body></html>",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "4.3.1. Random Optimization with Planar Constraint ",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "To evaluate the effectiveness and accuracy of the proposed planar-constrained stochastic optimization method for tracking fast camera movements, we compared stochastic optimization methods with and without planar constraints [4]. The method without planar constraints involves sampling particles directly around the best pose from the previous frame for particle fitness evaluation and iteratively computing APS to obtain the optimal solution. The iteration counts of the two methods for processing the same dataset were statistically analyzed on different test datasets, as shown in Figure 4. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Compared to the original approach, the introduction of planar constraints significantly reduces the number of iterations required to optimize each frame. This greatly improves the efficiency of the optimization process. Due to the CPS selection mechanism, the range of randomly selected particles is significantly narrowed, resulting in a set of particles that are closer to the optimal pose in the initial iteration and to some extent avoid getting trapped in local optima. Our implementation ensures that the random optimization process converges quickly with a minimum number of iterations. When we set the termination condition to “APS is empty for two consecutive iteration steps” and “the change of the optimal pose $T ^ { k ^ { * } }$ in 6DoF is less than $1 \\times 1 0 ^ { - 6 }$ for two consecutive iteration steps”, our optimization algorithm typically converges in less than two iterations for slow motion $( < 1 \\mathrm { m } / \\mathrm { s } )$ and in less than five iterations for fast motion $( \\ge 1 \\mathrm { m } / \\mathrm { s } )$ . ",
        "page_idx": 10
    },
    {
        "type": "image",
        "img_path": "images/28293ed788f7db2c6ac13d099ca47c77318faf395abc599ad10a1da42cc77078.jpg",
        "img_caption": [
            "Figure 4. Comparison of average iteration counts per frame between the two random optimization methods with and without the proposed planar constraint [4]. "
        ],
        "img_footnote": [],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "We further validated the efficiency of our implementation by calculating the average processing time per frame $t ,$ as shown in Table 3. Additionally, the table presents the Absolute Trajectory Error (ATE) between our method and the comparison methods against the ground truth trajectory, as well as the Mean Distance (MD) between the reconstructed and ground truth models. The Root Mean Square Error (RMSE) of the ATE for slow motion sequences was calculated using the tool provided by Sturm et al. [53]. Due to the difficulty of capturing such fast camera motion with a visual motion capture system, there is no ground truth trajectory available in the public datasets for our captured sequences. Hence, we indirectly assessed the accuracy of pose estimation by utilizing high-precision 3D dense laser-scanned reconstruction models provided by Zhang et al. [4] as ground truth. The accuracy evaluation was conducted by calculating the MD between the reconstructed model and the laser-scanned model, employing the open-source software CloudCompare version 2.11.3. ",
        "page_idx": 11
    },
    {
        "type": "table",
        "img_path": "images/466c9e6dbc59b2d896440b35a351dab80752ce3cec013a3ce89b198b22c465ec.jpg",
        "table_caption": [
            "Table 3. Comparing the efficiency and accuracy of pose estimation on whether to introduce planar constraint. The time and accuracy best results for each sequence are highlighted in blue color. "
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td rowspan=\"2\">Sequence</td><td colspan=\"2\">t (ms)</td><td colspan=\"2\">ATE (cm)</td><td colspan=\"2\">MD (cm)</td></tr><tr><td>RoseFusion</td><td>OwlFusion</td><td>RoseFusion</td><td>OwlFusion</td><td>RoseFusion</td><td>OwlFusion</td></tr><tr><td>fr1/desk</td><td>218.38</td><td>23.35</td><td>2.48</td><td>1.93</td><td></td><td></td></tr><tr><td>fr1/room</td><td>219.04</td><td>24.20</td><td>4.86</td><td>4.32</td><td></td><td></td></tr><tr><td>fr3/office</td><td>209.98</td><td>23.81</td><td>2.51</td><td>2.63</td><td></td><td></td></tr><tr><td>lr_kt0</td><td>214.63</td><td>24.73</td><td>0.83</td><td>0.77</td><td></td><td></td></tr><tr><td>lr_kt1</td><td>212.69</td><td>24.55</td><td>0.71</td><td>0.80</td><td></td><td></td></tr><tr><td>camera_shake1</td><td>224.24</td><td>27.09</td><td>0.62</td><td>0.93</td><td></td><td></td></tr><tr><td>camera_shake2</td><td>227.84</td><td>26.64</td><td>1.35</td><td>1.07</td><td></td><td></td></tr><tr><td>camera_shake3</td><td>232.18</td><td>29.39</td><td>4.67</td><td>4.54</td><td></td><td></td></tr><tr><td>synth/apartmentl</td><td>228.93</td><td>29.60</td><td>1.10</td><td>1.32</td><td>4.52</td><td>4.37</td></tr><tr><td>synth/hotel</td><td>230.62</td><td>30.08</td><td>1.52</td><td>1.33</td><td>5.25</td><td>5.54</td></tr><tr><td>real/lab</td><td>230.24</td><td>30.27</td><td></td><td></td><td>4.86</td><td>4.50</td></tr><tr><td>real/apartment1</td><td>230.75</td><td>30.51</td><td></td><td></td><td>4.88</td><td>5.45</td></tr><tr><td> real/apartment2</td><td>228.69</td><td>24.20</td><td></td><td></td><td>4.23</td><td>5.01</td></tr></table></body></html>",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Compared to the original random optimization method for pose estimation, the method with planar constraints improves the efficiency of pose estimation by about 8 times while maintaining the same level of accuracy. The time it takes to estimate the pose increases as the camera’s velocity increases since more optimization iteration steps are required to ensure accurate and stable tracking of fast-moving cameras. However, our pose estimation method and the comparison method do not have the same iteration time consumption, as this depends on the chosen strategy for particle sets and fitness evaluation. While we adopted the same approach to assess the particle fitness as the comparison method, we pre-selected a particle set with a smaller CPS of planar constraints to significantly reduce computational costs during the random optimization process and accelerate pose optimization convergence. The improvement in pose estimation efficiency is due to our estimation algorithm, as well as the GPU parallel computing and the hierarchical sparse data structure we used. It is worth noting that in our tests, we only used sequence frames that could be reconstructed with the same range as the comparison method. The reason for this is that the method lacked scene scalability, which we addressed in our implementation. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "4.3.2. Scalability and Quality of Scene Reconstruction ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "To evaluate the scalability of our proposed method, we compared its ability to reconstruct long RGB-D sequences with regular volume reconstruction approaches. Figure 5 presents and compares the reconstruction outcomes of both methods on real-world scenes captured by our MAV. The sequences’ ground length exceeded $3 0 \\mathrm { m } ,$ with corridor 1 and courtyard exceeding $5 0 \\mathrm { m }$ . Our proposed method produced complete reconstructions of the entire sequences in all three scenes, while the comparison method only provided partial reconstructions. This limitation stemmed from the regular volume surface reconstruction approach’s need to predefine the reconstruction range before surface reconstruction, as well as the limited GPU memory available in airborne computing devices, which resulted in a much smaller maximum reconstruction range than that of base stations. However, our approach did not require such presetting of the reconstruction range and enabled timely data exchange between the GPU and host via the adopted bi-directional GPU–Host data exchange approach, thus freeing up space for subsequent surface reconstruction. Additionally, the random optimization pose estimation in regular volume reconstruction experienced a decline in precision when the camera approached the edge of the predefined reconstruction boundaries, leading to reconstruction misalignment, as demonstrated by the red box in Figure 5. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "To validate whether the imaging and scan validity weighting introduced is useful in reducing sensor noise in deep fusion, we compared the reconstruction details of two methods, one with weighting and the other without. Figure 6 depicts the partial reconstruction details of two publicly available datasets. Our approach measures the observation validity in the deep fusion, thus obtaining high-quality denoising modeling from defective data. In an extreme case, as shown in Figure 7, there is a defect in scanning the left part of the teddy bear, causing significant noise on the reconstructed surface (as indicated by the red circle). On the contrary, our weighted fusion method demonstrated better modeling performance. ",
        "page_idx": 12
    },
    {
        "type": "image",
        "img_path": "images/e559dd57a2350ecb95d8825c73508e62ac9caaabae6f741c92489854883018f6.jpg",
        "img_caption": [
            "Figure 5. Gallery of 3D reconstruction results for the three real captured sequences by our MAV. ForFigure 5. Gallery of 3D reconstruction results for the three real captured sequences by our MAV. For each sequence, we compare the scalability of our reconstruction method and regular volume construction method (other modules are consistent with our system).circle). On the contrary, our weighted fusion method demonstcircle). On the contrary, our weighted fusion method demonst "
        ],
        "img_footnote": [],
        "page_idx": 13
    },
    {
        "type": "image",
        "img_path": "images/f660ecc9d5fb25beb6284881fc2180f118eb03d4e6d742eac3a0b549db187069.jpg",
        "img_caption": [
            "Figure 6. Gallery of 3D reconstruction details for the two publicly available datasets. In the compar-Figure 6. Gallery of 3D reconstruction details for the two publicly available datasets. In the comparison figure, the left side considers weighting by imaging and scan validity, while the right side does not. "
        ],
        "img_footnote": [],
        "page_idx": 13
    },
    {
        "type": "image",
        "img_path": "images/60ed29a45031636e25af2c0ecde80160386d6ace7f8340c70c7742fc1c4b181d.jpg",
        "img_caption": [
            "Figure 7. An extremely unfavorable observation: throughout the entire scanning sequence, the left Figure 7. An extremely unfavorable observation: throughout the entire scanning sequence, the left Figure 7. An extremely unfavorable observation: throughout the entire scanning sequence, the left side of the teddy bear’s body remains at a too far distance from the camera (Distance too far), devi-side of the teddy bear’s body remains at a too far distance from the camera (Distance too far), devi-side of the teddy bear’s body remains at a too far distance from the camera (Distance too far), deviated ated from the camera center (Off-center), or with a large scanning angle (Large anated from the camera center (Off-center), or with a large scanning angle (Large afrom the camera center (Off-center), or with a large scanning angle (Large angle). "
        ],
        "img_footnote": [],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "4.3.3. System Efficiency ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Real-time performance is a critical requirement in many applications, particularly in robotics technology. Table 4 presents typical frame rates achieved by our CUDA parallel processing implementation, measured on Nvidia Xavier NX GPU. In all cases, they significantly exceed the real-time performance of the comparison method [4] on airborne consumer-grade graphics hardware. Our system operates in real-time at a rate of $2 8 { - } 3 7 \\mathrm { H z }$ without enabling real-time visualization. When real-time visualization is enabled, the system’s efficiency decreases, but it still runs at a minimum rate of $3 0 \\mathrm { H z }$ . This is mainly due to the more complex interpolation scheme used in the hierarchical voxel representation during the ray-casting process [19]. The introduction of the effectiveness weighting increases the time consumption of the surface reconstruction step, which accounts for approximately $8 . 7 \\%$ of the total time consumption compared to the comparison method’s $7 . 5 \\%$ , which can be negligible. ",
        "page_idx": 14
    },
    {
        "type": "table",
        "img_path": "images/fa4da8b7954d1e35c63ad2788423d70524d1f3f7b0efcd389b71325c73018131.jpg",
        "table_caption": [
            "Table 4. Comparing the system running efficiency on different systems (the comparison method [4], our method with real-time visualization, and our method without real-time visualization), measured in Frames Per Second (FPS). "
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td rowspan=\"2\">Sequence</td><td colspan=\"3\">FPS (Hz)</td></tr><tr><td>Comparison</td><td>Ours w/ Vis.</td><td>Ours w/o Vis.</td></tr><tr><td>fr1/desk</td><td>3.50</td><td>30.19</td><td>37.18</td></tr><tr><td>fr1/room</td><td>3.49</td><td>29.14</td><td>35.88</td></tr><tr><td>fr3/ office</td><td>3.64</td><td>29.60</td><td>36.46</td></tr><tr><td>lr_kt0</td><td>3.56</td><td>28.51</td><td>35.11</td></tr><tr><td>lr_kt1</td><td>3.60</td><td>28.72</td><td>35.37</td></tr><tr><td>camera_shake1</td><td>3.41</td><td>26.02</td><td>32.05</td></tr><tr><td>camera_shake2</td><td>3.36</td><td>26.46</td><td>32.59</td></tr><tr><td>camera_shake3</td><td>3.29</td><td>23.99</td><td>29.54</td></tr><tr><td> synth/apartment1</td><td>3.34</td><td>23.82</td><td>29.34</td></tr><tr><td>synth/hotel</td><td>3.32</td><td>23.44</td><td>28.87</td></tr><tr><td>real/lab</td><td>3.32</td><td>23.29</td><td>28.68</td></tr><tr><td>real/apartmentl</td><td>3.32</td><td>23.11</td><td>28.46</td></tr><tr><td>real/apartment2</td><td>3.35</td><td>30.19</td><td>37.18</td></tr></table></body></html>",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "4.4. Comparison ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "In this section, we compared our proposed method with the state-of-the-art approaches in the field. We discussed the similarities, differences, strengths, and limitations of various methods, providing readers with a thorough and critical assessment. This comparative analysis helped readers understand the unique contributions and advantages of our proposed method in relation to other existing techniques. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "4.4.1. Quantitative Comparison ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "We quantitatively evaluated the performance of our method and several state-of-the-art methods on both normal and fast motion sequences. Table 5 compares our method with the state-of-the-art online ORB-SLAM2 [33], ElasticFusion [56], InfiniTAM [18], BundleFusion [3], and BAD-SLAM [55] on two sequences each from the slow-motion dataset, ICL-NUIM, and the fast-motion dataset, FastCaMo_synth, in terms of ATE. Our method achieved camera tracking accuracy comparable to the best-performing BundleFusion on the slow-motion sequences, which involves global pose optimization through Bundle Adjustment (BA), while our method does not involve any global pose optimization. As a result, our method slightly lags behind BundleFusion in terms of accuracy. Our method’s advantage is best demonstrated in the fast-motion sequences, in which the average speed of camera motion is over 10 times faster than that of ICL-NUIM. Other comparison methods failed to achieve successful reconstruction with these fast-motion datasets, whereas our method still achieved tracking accuracy with an ATE of less than $1 . 5 \\mathrm { { c m } }$ on all three sequences. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "This discrepancy arises due to the fact that the other methods rely on feature points or other photometric information for localization. However, in the case of fast motion, the resulting blurry RGB images pose challenges in providing relevant information for accurate localization. ",
        "page_idx": 15
    },
    {
        "type": "table",
        "img_path": "images/22c6b5d67deffcbd522bcb38065e7e47ce8bef32b6052085d51af557a90471df.jpg",
        "table_caption": [
            "Table 5. Comparing the ATE RMSE (cm) of camera tracking on the four RGB-D sequences from two datasets, ICL-NUIM and FastCaMo_synth. The best and the second-best results for each sequence are highlighted in blue and orange colors, respectively. "
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>Method</td><td>lr_kt0</td><td>lr_kt1</td><td> syn./apartment1</td><td>syn./hotel</td></tr><tr><td>ORB-SLAM2</td><td>1.11</td><td>0.46</td><td>一</td><td></td></tr><tr><td>ElasticFusion</td><td>1.06</td><td>0.82</td><td>41.09</td><td>43.64</td></tr><tr><td>InfiniTAM</td><td>0.89</td><td>0.67</td><td>10.38</td><td></td></tr><tr><td>BundleFusion</td><td>0.61</td><td>0.53</td><td>4.70</td><td>65.33</td></tr><tr><td>BAD-SLAM</td><td>1.73</td><td>1.09</td><td></td><td></td></tr><tr><td>OwlFusion</td><td>0.83</td><td>0.72</td><td>1.08</td><td>1.47</td></tr></table></body></html>",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "We compared the reconstruction performance of two synthetic sequences and three real capture sequences from the FastCaMo dataset in Table 6. The ground truth reconstruction of real capture sequences was obtained from high-precision Light Detection and Ranging (LiDAR) scans, and synthetic sequences also had ground truth surfaces of the same type. As a result, we evaluated the completeness, accuracy, and real-time performance of the reconstructed surfaces with regard to the ground truth surfaces. Since the reconstruction accuracy measures only the MD of overlap (inlier) areas between the reconstructed surfaces and ground truth surfaces, we set the inlier threshold to $1 5 \\mathrm { \\ c m }$ in Table 6. When the threshold was set to $5 \\mathrm { c m }$ , the average error was between 1 and $3 \\mathrm { c m } ,$ and the completeness decreased by about $1 0 \\%$ . Reconstruction quality is best reflected in completeness, and in this regard, our method is consistently superior to the two comparison methods, as shown in the visual results of the reconstruction presented in Figure 8. This is attributed to the fact that, on one hand, the comparative methods struggle to track fast motion accurately, and on the other hand, their re-localization modules struggle to function effectively solely based on blurry RGB images after tracking failure. Real-time performance is a focus of interest in the robotics community, and our system achieves an efficiency equivalent to that of InfiniTAM, the state-of-the-art high frame rate 3D reconstruction system, on mobile devices. Although the FPS is slightly lower than that of InfiniTAM, our method is significantly superior to it in terms of reconstruction completeness and accuracy. In contrast, BundleFusion requires more processing time to handle input frames on resource-limited mobile devices and even encounters program running failures due to the high computational overhead of running BundleFusion. ",
        "page_idx": 15
    },
    {
        "type": "table",
        "img_path": "images/8e251c5fc42a2912f3a8659831d68207933e08e675e9a49b5c8b5c6dfb91c481.jpg",
        "table_caption": [
            "Table 6. Comparing the reconstruction completeness (Compl. $\\%$ ), accuracy (Acc. cm), and running efficiency (FPS $_ \\mathrm { H z }$ ) on the five RGB-D sequences from the FastCaMo_synth and FastCaMo_real datasets. The best results for each sequence are highlighted in blue color. "
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td rowspan=\"2\"> Sequence</td><td colspan=\"3\">InfiniTAM</td><td colspan=\"3\">BundleFusion</td><td colspan=\"3\">OwlFusion</td></tr><tr><td>Compl.</td><td>Acc.</td><td>FPS</td><td>Compl.</td><td>Acc.</td><td>FPS</td><td>Compl.</td><td>Acc.</td><td>FPS</td></tr><tr><td> syn./apartment1</td><td>21.74</td><td>7.32</td><td>31.87</td><td>39.82</td><td>5.48</td><td>0.67</td><td>93.65</td><td>4.37</td><td>29.34</td></tr><tr><td>syn./hotel</td><td>33.13</td><td>6.98</td><td>28.33</td><td>47.64</td><td>4.90</td><td>0.42</td><td>94.57</td><td>5.54</td><td>28.87</td></tr><tr><td>real/lab</td><td>11.21</td><td>9.24</td><td>30.75</td><td>16.88</td><td>5.42</td><td></td><td>92.81</td><td>4.50</td><td>28.68</td></tr><tr><td> real/apartment1</td><td>9.83</td><td>8.73</td><td>29.37</td><td>34.23</td><td>6.39</td><td></td><td>87.23</td><td>5.45</td><td>28.46</td></tr><tr><td>real/apartment2</td><td>15.07</td><td>8.68</td><td>32.92</td><td>25.17</td><td>5.23</td><td></td><td>89.65</td><td>5.01</td><td>37.18</td></tr></table></body></html>",
        "page_idx": 15
    },
    {
        "type": "image",
        "img_path": "images/576bbb4f1c351037a3dced2477da737b524b978af1e61c86d0a7d3ed64ddc96f.jpg",
        "img_caption": [
            "Figure 8. Gallery of 3D reconstruction results for the three real captured sequences by our MAV. "
        ],
        "img_footnote": [],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Due to the almost compl4.4.2. Qualitative Comparison ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "In the qualitative comparative experiment, we present the visual results of 3D reconstruction for the fast-motion dataset. Figure 8 shows the reconstruction of three real capture sequences by our MAV. For each sequence, we display the reconstruction results sive camera motion results in severe image motion blur, making the loop closure of Bun-of OwlFusion (left), InfiniTAM (middle), and BundleFusion (right). It is evident that our dleFusion, which depends on color information, not work well. Although InfiniTAM suc-reconstruction is more complete and the surface quality is acceptable compared to the comcessfully detects loops using depth images, it still cannot correct the significant accumu-parison methods. Although BundleFusion failed to succeed on these fast-motion sequences, lation error caused by fast camera motion. In contrast to these frame-to-frame posthe few frames with successful tracking correspond to a well-reconstructed surface. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "on methods, our motion tracking uses a frame-to-model method, which has higherDue to the almost complete lack of loops in our dataset, which is a problem that robots robustness and accuracy [2,56]. This makes our system much less prone to drift in fastmust face directly in their actual applications, and our desire not to complete too much motion at the same distance than some frame-to-frame methods, as shown in Tables 5 andredundant work, even though both comparative methods have re-localization modules, it is 6. However, the accuracy of the frame-to-model pose estimation method depends on an difficult to recover reconstruction after losing tracking. Additionally, the aggressive camera accurate model, which forces us to pay more attention to the surface quality of reconstruc-motion results in severe image motion blur, making the loop closure of BundleFusion, tion. Despite this, significant drift still occurs in the reconstruction of extremely long-dis-which depends on color information, not work well. Although InfiniTAM successfully tance scenes. detects loops using depth images, it still cannot correct the significant accumulation error caused by fast camera motion. In contrast to these frame-to-frame pose estimation methods, 5. Limitations our motion tracking uses a frame-to-model method, which has higher robustness and Although our method has achieved excellent scalable real-time 3D reconstruction accuracy [2,56]. This makes our system much less prone to drift in fast motion at the same performance on low-compute devices, it still has some limitations. Firstly, the efficiencydistance than some frame-to-frame methods, as shown in Tables 5 and 6. However, the of our system just meets the “real-time” requirement, i.e., a frame rate of 30 Hz for image accuracy of the frame-to-model pose estimation method depends on an accurate model, processing, which makes it impossible to simultaneously perform scene reconstruction which forces us to pay more attention to the surface quality of reconstruction. Despite this, and path planning algorithms onboard devices. Additionally, our method maysignificant drift still occurs in the reconstruction of extremely long-distance scenes. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "5. Limitations ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": ". Furthermore, within this paper, our focus has been primarily on the geometric re-Although our method has achieved excellent scalable real-time 3D reconstruction construction of scenes. However, it is crucial to acknowledge that rich texture information performance on low-compute devices, it still has some limitations. Firstly, the efficiency plays a pivotal role in achieving high-fidelity 3D reconstruction. Unforof our system just meets the “real-time” requirement, i.e., a frame rate of $3 0 \\mathrm { H z }$ ly, blurry for image RGB images pose a challenge in providing high-quality texture mapping. Finally, our sys-processing, which makes it impossible to simultaneously perform scene reconstruction tem lacks the capability of global model optimization, which limits the application of high-and path planning algorithms onboard devices. Additionally, our method may suffer quality ultra-long distance scene reconstruction. We believe that these limitations are from pose estimation errors in scenes with degraded geometric features, such as graffiti worth exploring in future research. walls. Joint estimation of geometric and photometric information may be a possible solution. Furthermore, within this paper, our focus has been primarily on the geometric 6. Conclusions reconstruction of scenes. However, it is crucial to acknowledge that rich texture information plays a pivotal role in achieving high-fidelity 3D reconstruction. Unfortunately, blurry RGB images pose a challenge in providing high-quality texture mapping. Finally, our method on MAVs. Our approach leverages two main design choices to achieve satisfactory system lacks the capability of global model optimization, which limits the application of results. Firstly, we introduced planar constraints in the random particle selection process high-quality ultra-long distance scene reconstruction. We believe that these limitations are by computing partition normal maworth exploring in future research. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "6. Conclusions ",
        "text_level": 1,
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "In this work, we proposed a depth-only onboard scalable real-time 3D reconstruction method on MAVs. Our approach leverages two main design choices to achieve satisfactory results. Firstly, we introduced planar constraints in the random particle selection process by computing partition normal maps, reducing the computational cost of the random optimization and improving the efficiency of pose estimation. Secondly, we combined the hierarchical voxel hashing function with particle swarm optimization to further reduce the computational burden and storage cost of onboard devices, enabling real-time reconstruction of large-scale scenes. Lastly, we considered the validity of camera scanning and imaging and quantified it before incorporating it into the depth data fusion process to control the noise impact on the reconstructed surface. Future research can address the limitations of our approach, such as increasing the global model optimization capability, further improving the robustness of pose estimation while reducing the demand for computational resources, and expanding the application scenarios. Additionally, integrating other sensor data, such as stereo cameras and LiDAR, can be explored to further improve the accuracy and robustness of 3D reconstruction. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Author Contributions: Conceptualization, G.G.; methodology, G.G.; software, G.G. and X.W.; validation, G.G., X.W. and S.W.; formal analysis, G.G.; investigation, H.Z. and J.L.; resources, H.S.; data curation, G.G. and X.W.; writing—original draft preparation, G.G.; writing—review and editing, G.G., H.Z. and J.L.; visualization, G.G. and S.W.; supervision, H.S.; project administration, H.S. and G.G.; funding acquisition, H.S. All authors have read and agreed to the published version of the manuscript. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Funding: The research was funded by the Natural Science Foundation of China (NSFC) Major Program (42192580, 42192583) and the Guangxi Science and Technology Major Project (No. AA22068072). ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Data Availability Statement: The data presented in this study are available on request from the corresponding author. The data are not publicly available due to privacy. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Conflicts of Interest: The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "1. Newcombe, R.A.; Izadi, S.; Hilliges, O.; Molyneaux, D.; Kim, D.; Davison, A.J.; Kohli, P.; Shotton, J.; Hodges, S.; Fitzgibbon, A.W. KinectFusion: Real-time dense surface mapping and tracking. In Proceedings of the 2011 10th IEEE International Symposium on Mixed and Augmented Reality, Basel, Switzerland, 26–29 October 2011; pp. 127–136.   \n2. Whelan, T.; Kaess, M.; Fallon, M.F.; Johannsson, H.; Leonard, J.J.; McDonald, J.B. Kintinuous: Spatially Extended KinectFusion. In Proceedings of the AAAI Conference on Artificial Intelligence, Karlsruhe, Germany, 6–10 May 2013.   \n3. Dai, A.; Nießner, M.; Zollhöfer, M.; Izadi, S.; Theobalt, C. BundleFusion: Real-time globally consistent 3D reconstruction using on-the-fly surface re-integration. ACM Trans. Graph. 2016, 36, 1. [CrossRef]   \n4. Zhang, J.; Zhu, C.; Zheng, L.; Xu, K. ROSEFusion: Random Optimization for Online Dense Reconstruction under Fast Camera Motion. ACM Trans. Graph. 2021, 40, 1–17.   \n5. Newcombe, R.A.; Lovegrove, S.; Davison, A.J. DTAM: Dense tracking and mapping in real-time. In Proceedings of the 2011 International Conference on Computer VIsion, Barcelona, Spain, 6–13 November 2011; pp. 2320–2327.   \n6. Curless, B.; Levoy, M. A volumetric method for building complex models from range images. In Proceedings of the 3rd Annual Conference on Computer Graphics and Interactive Techniques, New York, NY, USA, 14–19 July 1996.   \n7. Nießner, M.; Zollhöfer, M.; Izadi, S.; Stamminger, M. Real-time 3D reconstruction at scale using voxel hashing. ACM Trans. Graph. 2013, 32, 1–11. [CrossRef]   \n8. Zeng, M.; Zhao, F.; Zheng, J.; Liu, X. Octree-based fusion for realtime 3D reconstruction. Graph. Model. 2013, 75, 126–136. [CrossRef]   \n9. Chen, J.; Bautembach, D.; Izadi, S. Scalable real-time volumetric surface reconstruction. ACM Trans. Graph. 2013, 32, 1–16. [CrossRef]   \n10. Steinbrücker, F.; Sturm, J.; Cremers, D. Volumetric 3D mapping in real-time on a CPU. In Proceedings of the 2014 IEEE International Conference on Robotics and Automation (ICRA), Hong Kong, China, 31 May–7 June 2014; pp. 2021–2028.   \n11. Dahl, V.A.; Aanæs, H.; Bærentzen, J.A. Surfel Based Geometry Reconstruction. In Proceedings of the TPCG, Sheffield, UK, 6–8 September 2010.   \n12. Keller, M.; Lefloch, D.; Lambers, M.; Izadi, S.; Weyrich, T.; Kolb, A. Real-Time 3D Reconstruction in Dynamic Scenes Using Point-Based Fusion. In Proceedings of the 2013 International Conference on 3D Vision-3DV, Seattle, WA, USA, 29 June–1 July 2013; pp. 1–8.   \n13. Salas-Moreno, R.F.; Glocker, B.; Kelly, P.H.J.; Davison, A.J. Dense planar SLAM. In Proceedings of the 2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), Munich, Germany, 10–12 September 2014; pp. 157–164.   \n14. Whelan, T.; Salas-Moreno, R.F.; Glocker, B.; Davison, A.J.; Leutenegger, S. ElasticFusion: Real-time dense SLAM and light source estimation. Int. J. Rob. Res. 2016, 35, 1697–1716. [CrossRef]   \n15. Prisacariu, V.A.; Kähler, O.; Golodetz, S.; Sapienza, M.; Cavallari, T.; Torr, P.H.S.; Murray, D.W. InfiniTAM v3: A Framework for Large-Scale 3D Reconstruction with Loop Closure. arXiv 2017, arXiv:abs/1708.0.   \n16. Mandikal, P.; Babu, R.V. Dense 3D Point Cloud Reconstruction Using a Deep Pyramid Network. In Proceedings of the 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), Waikoloa, HI, USA, 7–11 January 2019; pp. 1052–1060.   \n17. Mihajlovi´c, M.; Weder, S.; Pollefeys, M.; Oswald, M.R. DeepSurfels: Learning Online Appearance Fusion. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20–25 June 2020; pp. 14519–14530.   \n18. Kähler, O.; Prisacariu, V.A.; Ren, C.Y.; Sun, X.; Torr, P.H.S.; Murray, D.W. Very High Frame Rate Volumetric Integration of Depth Images on Mobile Devices. IEEE Trans. Vis. Comput. Graph. 2015, 21, 1241–1250. [CrossRef]   \n19. Kähler, O.; Prisacariu, V.A.; Valentin, J.P.C.; Murray, D.W. Hierarchical Voxel Block Hashing for Efficient Integration of Depth Images. IEEE Robot. Autom. Lett. 2016, 1, 192–197. [CrossRef]   \n20. Huang, A.S.; Bachrach, A.; Henry, P.; Krainin, M.; Maturana, D.; Fox, D.; Roy, N. Visual Odometry and Mapping for Autonomous Flight Using an RGB-D Camera. In Proceedings of the International Symposium of Robotics Research, Flagstaff, AZ, USA, 28 August–1 September 2011.   \n21. Fraundorfer, F.; Heng, L.; Honegger, D.; Lee, G.H.; Meier, L.; Tanskanen, P.; Pollefeys, M. Vision-based autonomous mapping and exploration using a quadrotor MAV. In Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, Vilamoura-Algarve, Portugal, 7–12 October 2012; pp. 4557–4564.   \n22. Bachrach, A.; Prentice, S.; He, R.; Henry, P.; Huang, A.S.; Krainin, M.; Maturana, D.; Fox, D.; Roy, N. Estimation, planning, and mapping for autonomous flight using an RGB-D camera in GPS-denied environments. Int. J. Rob. Res. 2012, 31, 1320–1343. [CrossRef]   \n23. Bylow, E.; Sturm, J.; Kerl, C.; Kahl, F.; Cremers, D. Real-Time Camera Tracking and 3D Reconstruction Using Signed Distance Functions. In Proceedings of the Robotics: Science and Systems, Berlin, Germany, 24–28 June 2013.   \n24. Heng, L.; Honegger, D.; Lee, G.H.; Meier, L.; Tanskanen, P.; Fraundorfer, F.; Pollefeys, M. Autonomous Visual Mapping and Exploration With a Micro Aerial Vehicle. J. F Robot. 2014, 31, 654–675. [CrossRef]   \n25. Burri, M.; Oleynikova, H.; Achtelik, M.; Siegwart, R.Y. Real-time visual-inertial mapping, re-localization and planning onboard MAVs in unknown environments. In Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Hamburg, Germany, 28 September–2 October 2015; pp. 1872–1878.   \n26. Zhao, X.; Chong, J.; Qi, X.; Yang, Z. Vision Object-Oriented Augmented Sampling-Based Autonomous Navigation for Micro Aerial Vehicles. Drones 2021, 5, 107. [CrossRef]   \n27. Chen, C.; Wang, Z.; Gong, Z.; Cai, P.; Zhang, C.; Li, Y. Autonomous Navigation and Obstacle Avoidance for Small VTOL UAV in Unknown Environments. Symmetry 2022, 14, 2608. [CrossRef]   \n28. Hao, C.K.; Mayer, N. Real-time SLAM using an RGB-D camera for mobile robots. In Proceedings of the 2013 CACS International Automatic Control Conference (CACS), Nantou, Taiwan, China, 2–4 December 2013; pp. 356–361.   \n29. Nowicki, M.R.; Skrzypczy ´nski, P. Combining photometric and depth data for lightweight and robust visual odometry. In Proceedings of the 2013 European Conference on Mobile Robots, Barcelona, Spain, 25–27 September 2013; pp. 125–130.   \n30. Saeedi, S.; Nagaty, A.; Thibault, C.; Trentini, M.; Li, H. 3D Mapping and Navigation for Autonomous Quadrotor Aircraft. In Proceedings of the IEEE 29th Canadian Conference on Electrical and Computer Engineering (CCECE), Vancouver, BC, Canada, 15–18 May 2016.   \n31. Aguilar, W.G.; Rodríguez, G.A.; Álvarez, L.G.; Sandoval, S.; Quisaguano, F.J.; Limaico, A. Visual SLAM with a RGB-D Camera on a Quadrotor UAV Using on-Board Processing. In Proceedings of the International Work-Conference on Artificial and Natural Neural Networks, Cádiz, Spain, 14–16 June 2017.   \n32. Mur-Artal, R.; Montiel, J.M.M.; Tardós, J.D. ORB-SLAM: A Versatile and Accurate Monocular SLAM System. IEEE Trans. Robot. 2015, 31, 1147–1163. [CrossRef]   \n33. Mur-Artal, R.; Tardós, J.D. ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras. IEEE Trans. Robot. 2016, 33, 1255–1262. [CrossRef]   \n34. Handa, A.; Newcombe, R.A.; Angeli, A.; Davison, A.J. Real-Time Camera Tracking: When is High Frame-Rate Best? In Proceedings of the European Conference on Computer Vision, Florence, Italy, 7–13 October 2012.   \n35. Zhang, Q.; Huang, N.; Yao, L.; Zhang, D.; Shan, C.; Han, J. RGB-T Salient Object Detection via Fusing Multi-Level CNN Features. IEEE Trans. Image Process. 2019, 29, 3321–3335. [CrossRef] [PubMed]   \n36. Saurer, O.; Pollefeys, M.; Lee, G.H. Sparse to Dense 3D Reconstruction from Rolling Shutter Images. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27–30 June 2016; pp. 3337–3345.   \n37. Gallego, G.; Delbrück, T.; Orchard, G.; Bartolozzi, C.; Taba, B.; Censi, A.; Leutenegger, S.; Davison, A.J.; Conradt, J.; Daniilidis, K.; et al. Event-Based Vision: A Survey. IEEE Trans. Pattern Anal. Mach. Intell. 2019, 44, 154–180. [CrossRef] [PubMed]   \n38. Lee, H.S.; Kwon, J.; Lee, K.M. Simultaneous localization, mapping and deblurring. In Proceedings of the 2011 International Conference on Computer Vision, Barcelona, Spain, 6–13 November 2011; pp. 1203–1210.   \n39. Zhang, H.; Yang, J. Intra-frame deblurring by leveraging inter-frame camera motion. In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7–15 June 2015; pp. 4036–4044.   \n40. Forster, C.; Carlone, L.; Dellaert, F.; Scaramuzza, D. On-Manifold Preintegration for Real-Time Visual–Inertial Odometry. IEEE Trans. Robot. 2015, 33, 1–21. [CrossRef]   \n41. Xu, C.; Liu, Z.; Li, Z. Robust Visual-Inertial Navigation System for Low Precision Sensors under Indoor and Outdoor Environments. Remote Sens. 2021, 13, 772. [CrossRef]   \n42. Nießner, M.; Dai, A.; Fisher, M. Combining Inertial Navigation and ICP for Real-time 3D Surface Reconstruction. In Proceedings of the Eurographics, Strasbourg, French, 7–11 April 2014.   \n43. Prisacariu, V.A.; Kähler, O.; Murray, D.W.; Reid, I.D. Real-Time 3D Tracking and Reconstruction on Mobile Phones. IEEE Trans. Vis. Comput. Graph. 2015, 21, 557–570. [CrossRef] [PubMed]   \n44. Laidlow, T.; Bloesch, M.; Li, W.; Leutenegger, S. Dense RGB-D-inertial SLAM with map deformations. In Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Vancouver, BC, Canada, 24–28 September 2017; pp. 6741–6748.   \n45. Hansard, M.; Lee, S.; Choi, O.; Horaud, R. Time of Flight Cameras: Principles, Methods, and Applications; Springer: Berlin/Heidelberg, Germany, 2012.   \n46. Ji, C.; Zhang, Y.; Tong, M.; Yang, S. Particle Filter with Swarm Move for Optimization. In Proceedings of the Parallel Problem Solving from Nature, Dortmund, Germany, 13–17 September 2008.   \n47. Besl, P.J.; McKay, N.D. Method for registration of 3-D shapes. In Proceedings of the Other Conferences, Boston, MA, USA, 30 April 1992.   \n48. Kerl, C.; Sturm, J.; Cremers, D. Robust odometry estimation for RGB-D cameras. In Proceedings of the 2013 IEEE International Conference on Robotics and Automation, Karlsruhe, Germany, 6–10 May 2013; pp. 3748–3754.   \n49. Ginzburg, D.; Raviv, D. Deep Weighted Consensus: Dense correspondence confidence maps for 3D shape registration. arXiv 2021, arXiv:abs/2105.0.   \n50. Lu, Y.; Song, D. Robust RGB-D Odometry Using Point and Line Features. In Proceedings of the 2015 IEEE International Conference on Computer Vision, Santiago, Chile, 1–13 December 2015; pp. 3934–3942.   \n51. Yunus, R.; Li, Y.; Tombari, F. ManhattanSLAM: Robust Planar Tracking and Mapping Leveraging Mixture of Manhattan Frames. In Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA), Xi’an, China, 30 May–5 June 2021; pp. 6687–6693.   \n52. Zhu, Z.; Xu, Z.; Chen, R.; Wang, T.; Wang, C.; Yan, C.C.; Xu, F. FastFusion: Real-Time Indoor Scene Reconstruction with Fast Sensor Motion. Remote Sens. 2022, 14, 3551. [CrossRef]   \n53. Sturm, J.; Engelhard, N.; Endres, F.; Burgard, W.; Cremers, D. A benchmark for the evaluation of RGB-D SLAM systems. In Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, Vilamoura-Algarve, Portugal, 7–12 October 2012; pp. 573–580.   \n54. Handa, A.; Whelan, T.; McDonald, J.B.; Davison, A.J. A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM. In Proceedings of the 2014 IEEE International Conference on Robotics and Automation (ICRA), Hong Kong, China, 31 May–7 June 2014; pp. 1524–1531.   \n55. Schöps, T.; Sattler, T.; Pollefeys, M. BAD SLAM: Bundle Adjusted Direct RGB-D SLAM. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15–20 June 2019; pp. 134–144.   \n56. Whelan, T.; Leutenegger, S.; Salas-Moreno, R.F.; Glocker, B.; Davison, A.J. ElasticFusion: Dense SLAM Without A Pose Graph. In Proceedings of the Robotics: Science and Systems, Rome, Italy, 17 July 2015. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. ",
        "page_idx": 19
    }
]